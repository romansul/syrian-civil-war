{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for requesting content of a web page\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "# for regex\n",
    "# import re\n",
    "\n",
    "# for dataframe creation\n",
    "import pandas as pd\n",
    "\n",
    "# for not overwhelming server when scraping pages\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing One Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname=&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status=&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=0'\n",
    "response = get(url)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = html_soup.find('tr', {'title':'victim'})\n",
    "# print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = row.findAll('td')\n",
    "\n",
    "# prints out the nested text\n",
    "for i in range(len(columns)):\n",
    "    # skips first element b/c not useful\n",
    "    if i == 0:\n",
    "        continue\n",
    "    else:\n",
    "        print(columns[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for One Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the lists that contain information for each column \n",
    "# into a signle list to make access easier\n",
    "data_list = []\n",
    "\n",
    "# lists to store scraped data in\n",
    "victim_id = []\n",
    "data_list.append(victim_id)\n",
    "\n",
    "first_name = []\n",
    "data_list.append(first_name)\n",
    "\n",
    "father_name = []\n",
    "data_list.append(father_name)\n",
    "\n",
    "last_name = []\n",
    "data_list.append(last_name)\n",
    "\n",
    "city = []\n",
    "data_list.append(city)\n",
    "\n",
    "town = []\n",
    "data_list.append(town)\n",
    "\n",
    "date = []\n",
    "data_list.append(date)\n",
    "\n",
    "# preparation before scraping\n",
    "url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname=&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status=&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=0'\n",
    "response = get(url)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "rows = html_soup.findAll('tr', {'title':'victim'})\n",
    "\n",
    "# start scraping one page\n",
    "for row in rows:\n",
    "    columns = row.findAll('td')\n",
    "    \n",
    "    # saves the column data\n",
    "    for i in range(len(columns)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            data_list[i - 1].append(columns[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creates a data frame\n",
    "victim_info = pd.DataFrame({\n",
    "    'victim_id': data_list[0],\n",
    "    'first_name': data_list[1],\n",
    "    'father_name': data_list[2],\n",
    "    'last_name': data_list[3],\n",
    "    'province': data_list[4],\n",
    "    'town': data_list[5],\n",
    "    'date': data_list[6]    \n",
    "})\n",
    "\n",
    "victim_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_url = [str(i) for i in range(0, 91901, 50)]\n",
    "# print(numbers_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the lists that contain information for each column \n",
    "# into a signle list to make access easier\n",
    "data_list = []\n",
    "\n",
    "# lists to store scraped data in\n",
    "victim_id = []\n",
    "data_list.append(victim_id)\n",
    "\n",
    "first_name = []\n",
    "data_list.append(first_name)\n",
    "\n",
    "father_name = []\n",
    "data_list.append(father_name)\n",
    "\n",
    "last_name = []\n",
    "data_list.append(last_name)\n",
    "\n",
    "province = []\n",
    "data_list.append(province)\n",
    "\n",
    "town = []\n",
    "data_list.append(town)\n",
    "\n",
    "date = []\n",
    "data_list.append(date)\n",
    "\n",
    "# preparation before scraping\n",
    "for number_url in numbers_url:\n",
    "    url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname=' + \\\n",
    "            '&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status=' + \\\n",
    "            '&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3' + \\\n",
    "            '&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=' + \\\n",
    "            '&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=' + number_url\n",
    "    response = get(url)\n",
    "    \n",
    "#     sleep(randint(8,15))\n",
    "    sleep(1)\n",
    "    \n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    rows = html_soup.findAll('tr', {'title':'victim'})\n",
    "\n",
    "    # start scraping one page\n",
    "    for row in rows:\n",
    "        columns = row.findAll('td')\n",
    "\n",
    "        # saves the column data\n",
    "        for i in range(len(columns)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            else:\n",
    "                data_list[i - 1].append(columns[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a data frame\n",
    "victim_info = pd.DataFrame({\n",
    "    'victim_id': data_list[0],\n",
    "    'first_name': data_list[1],\n",
    "    'father_name': data_list[2],\n",
    "    'last_name': data_list[3],\n",
    "    'province': data_list[4],\n",
    "    'town': data_list[5],\n",
    "    'date': data_list[6]    \n",
    "})\n",
    "\n",
    "victim_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
