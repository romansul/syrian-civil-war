{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casualties and Migration in the Syrian Civil War"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "***\n",
    "\n",
    "In 2011, five weeks into the civil demonstrations against the Syrian government, secret police forces detained and tortured fifteen students who had spray painted an anti-government statement on the walls of their school. They would be released weeks later in an effort to quell the rising civil unrest in the province. In the wake of the hundreds of other demonstrators who were killed or disappeared, this action was too little and too late to stop the tide of the civil war. Demonstrations turned to protest turned to armed conflict and the rest is history.\n",
    "\n",
    "The war would go on to spawn both the largest refugee crisis and one of the deadliest conflicts in modern history. As of 2019, there are over 6 million Syrian refugees and another 6 million internally displaced people in a country with a pre-war population of around 24 million (UNHCR, 2018). The regime's efforts to prevent accurate information from leaving the country has made it nearly impossible to estimate the number of casualties that have occured in that time. Current estimates range from 300,000 to 600,000 killed depending on the source.\n",
    "\n",
    "The link between the flow of violence within the country and the flow of asylum seekers out of the country should be apparent to anyone who is aware of the war. Yet a growing sentiment among residents in host countries is that a large portion of asylum seekers from Syria are actually economic migrants, who are using the conflict as a means of gaining entry into the European Union and access to generous social programs.\n",
    "\n",
    "We believe that violence is the most important predictor of migration of Syrian refugees; however, while this argument may be generally accepted, there is great difficulty in proving this relationship for certain. We hope to answer this question using reported casualty data to see whether there is a correlation between violence in a given province and a subsequent increase in the amount of asylum seekers across all host countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "***\n",
    "\n",
    "Our project can be organized into three distinct portions:\n",
    "\n",
    "1. Data Scraping\n",
    "2. Data Wrangling\n",
    "3. Data Visualization\n",
    "\n",
    "Our goal is to create a dataset for casualty information a refugee data, clean and structure the dataset for easier queying, and visualize the data to provide more insights into the questions we pose above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "***\n",
    "\n",
    "There are multiple sources that could be used for casualty information (list here). We will leave the three datasets for now, and focus on the VDC and CSR datasets because they provide their data is table elements that make it easy for us to scrape and organize our dataframes for analysis.\n",
    "\n",
    "We will now go through the process of scraping and creating the inital forms of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Violations Documentation Center](http://www.vdc-sy.info/) has been recording casualty data since June 2011. It is likely the most detailed and complete (in terms of metadata) data source of casualties that is publicly accessible.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `Name                  - Full name in English`\n",
    "- `Status                - Civilian, non-civilian, or military status of deceased`\n",
    "- `Sex                   - Whether deceased is an Adult or Minor and Male or Female`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Area \\ Place of Birth - Various locations that can be Provinces/Subdistricts/Towns`\n",
    "- `Date of death         - self explanatory`\n",
    "- `Cause of death        - self explanatory`\n",
    "- `Actors                - groups involved in the casualty`\n",
    "\n",
    "Each entry is associated with a unique identifier, which is an integer between 0 and 250,000. Clicking on the name of the entry will lead the user to another page that provides the unique identifier number and other data that is not displayed on the main page. We will avoid describing this detail for now, since most of this data is not used in the final product.\n",
    "\n",
    "We will describe the full process we used to scrape all details from this website as well as the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent():\n",
    "    first_page = 'http://www.vdc-sy.info/index.php/en/martyrs/1/c29ydGJ5PWEua2lsbGVkX2RhdGV8c29ydGRpcj1ERVNDfGFwcHJvdmVkPXZpc2libGV8ZXh0cmFkaXNwbGF5PTB8'\n",
    "    \n",
    "    # This is the format of the links that give us the unique identfiers\n",
    "    pattern    = re.compile('\\/index\\.php\\/en\\/details\\/martyrs\\/.')\n",
    "\n",
    "    # We want to establish a randomized user agent and Tor node to avoid detection\n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    tor        = TorRequest(password = 'commonhorse')\n",
    "    \n",
    "    try:\n",
    "        response = tor.get(first_page, headers=headers)\n",
    "        content  = bs(response.text, 'html.parser')\n",
    "        \n",
    "        # This list comprehension grabs all unique identifiers in string format for all links that match\n",
    "        # our regex pattern from above\n",
    "        links    = {link['href'][30:] for link in content.find_all('a', href = True) if pattern.match(link['href'])} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Provided a list of unique identifiers in string fromat, scrapes details and saves each entry \n",
    "as an idividual dataframe that represents one person.\n",
    "'''\n",
    "\n",
    "def scrape_details(uid, tor, headers):\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    url  = 'http://www.vdc-sy.info/index.php/en/details/martyrs/' + uid\n",
    "    \n",
    "    # Headers will provide the UserAgent to use when getting response\n",
    "    # Makes the request using a TorRequest object passed in\n",
    "    page = tor.get(url, headers = headers).text\n",
    "    page = bs(page, 'html.parser')\n",
    "    \n",
    "    # Grabs the relevant table info and all rows in it\n",
    "    table = page.find('table', attrs = {'class':'peopleListing'})\n",
    "    rows  = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "\n",
    "        # All data without only 2 data values\n",
    "        # are not data we are looking for\n",
    "        if len(data) != 2:\n",
    "            continue\n",
    "\n",
    "        # data[0] corresponds to the row label/column\n",
    "        cols.append(data[0].text)\n",
    "        \n",
    "        # Values need to appended differently for image rows \n",
    "        if data[1].find('img') is not None:\n",
    "            vals.append(data[1].find('img')['src'])\n",
    "        else:\n",
    "            vals.append(data[1].text)\n",
    "\n",
    "    # Adds the uid to the dataframe\n",
    "    cols.append('uid')\n",
    "    vals.append(uid)\n",
    "\n",
    "    # Creates and saves dataframe\n",
    "    person = pd.DataFrame([vals], columns = cols, dtype=str)\n",
    "\n",
    "    save(person, os.path.join('person_dfs', uid))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each detailed page has a different number of columns depending on the metadata associated with that entry, so we will now have to combine all the dataframes. Pandas requires that columns have unique names, so we have to rename all duplicate columns using this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dup_cols(dataframe):\n",
    "    cols = pd.Series(dataframe.columns)\n",
    "  \n",
    "    for dup in dataframe.columns.get_duplicates(): \n",
    "        cols[dataframe.columns.get_loc(dup)] = [dup + '_' + str(d_idx) if d_idx != 0 else dup for d_idx in range(dataframe.columns.get_loc(dup).sum())]\n",
    "   \n",
    "    dataframe.columns = cols\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given a list of dataframes we can return a combined dataframe that retains all column data and saves that file as vdc_df and saves any failed dataframes as failed_vdc_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes):\n",
    "    failed_dataframes = []\n",
    "    combined          = pd.DataFrame()\n",
    "\n",
    "    current = 0\n",
    "    num     = len(dataframes)\n",
    "\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            combined = pd.concat([combined, df], axis = 0)\n",
    "            print(f'{counter} / {num} people processed in combine_dataframes().')\n",
    "            counter += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_dataframes.append(df)\n",
    "            print('Failed')\n",
    "            counter += 1\n",
    "\n",
    "    save(combined, 'vdc_df')\n",
    "    save(failed_dataframes, 'failed_vdc_df')\n",
    "\n",
    "    print('\\n\\nSuccess: ', len(dataframes) - len(failed_dataframes))\n",
    "    print('Failed: ', len(failed_dataframes))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding this all together. We will now:\n",
    "\n",
    "1. Build a list of unique identifiers by scraping the query page for the VDC database using scrape_recent()\n",
    "\n",
    "2. Scrape the detailed information provided the list of unique ids from scrape_recent() using scrape_details, which gives us dataframes for each person.\n",
    "\n",
    "3. Combine those dataframes into one large dataset using combine_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_to_scrape = scrape_recent()\n",
    "uids_scraped   = set()\n",
    "\n",
    "while len(uids_to_scrape) > 0:\n",
    "    uid = uids_to_scrape.pop()\n",
    "    \n",
    "    try:\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        scrape_details(uid, tor, headers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        helen___uids_to_scrape.append(uid)\n",
    "\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        tor.reset_identity()\n",
    "\n",
    "        continue\n",
    "        \n",
    "    uids_scraped.add(uid)\n",
    "\n",
    "    save(uids_to_scrape, 'uids_to_scrape')\n",
    "    save(uids_scraped  , 'uids_scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "\n",
    "for person_df in glob.glob(os.path.join('person_dfs', '*.pickle')):\n",
    "    list_of_dataframes.append(load(person_df))\n",
    "    \n",
    "combine_dataframes(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Syrian Center for Statistics and Research](https://csr-sy.org/) has been recording casualty data since March 2011. It has less information than the VDC dataset, but the location of death is more precise.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `ID Number             - Arbitrary ID number`\n",
    "- `First Name            - First name in Arabic`\n",
    "- `Father Name           - Father's last name in Arabic`\n",
    "- `Last Name             - Last name in Arabic`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Town                  - Town where they died`\n",
    "- `Date of death         - self explanatory`\n",
    "\n",
    "If you looked at the code to scrape the VDC website, you'll see that there is no package for Tor or user agent. For some reason, this particular website was cautious about who was looking at their data as it blocked our multiple attempts of trying to scrape without cycling through IP addresses and user agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contain all the libraries that must be imported in order to run the web scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for requesting content of a web page\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "# for regex\n",
    "# import re\n",
    "\n",
    "# for dataframe creation\n",
    "import pandas as pd\n",
    "\n",
    "# for not overwhelming server when scraping pages\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "# For a progress bar while scraping\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For saving files\n",
    "import pickle\n",
    "\n",
    "# For shuffling list\n",
    "import random\n",
    "\n",
    "# For Tor Requests\n",
    "from torrequest     import TorRequest\n",
    "\n",
    "# To cycle through useragents\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not have to rescrape everything if one page fails, it is essential to have a pickle file that you can store your successfully scraped data and failed attempts in. Below is the code to create and load a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load functions for a pickle file\n",
    "def save(obj, name):\n",
    "    pickle.dump(obj, open(name + '.pickle', 'wb'))\n",
    "\n",
    "def load(name):\n",
    "    return pickle.load(open(name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url to start scraping CSR\n",
    "url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname=&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status=&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=0'\n",
    "\n",
    "response = get(url)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website was odd in the sense that some pages at the end didn't contain any information about Syrian casualties. We found the last page that had any information and hard-coded the page number in. As you can see from the code below, 91900 was the last page to contain any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of numbers used in shuffling through all the URLs\n",
    "numbers_url = [str(i) for i in range(0, 91901, 50)]\n",
    "\n",
    "# shuffling the numbers so that the website doesn't become suspicious of us\n",
    "random.shuffle(numbers_url)\n",
    "numbers_url = set(numbers_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the pickle files: the good, the bad, and the ugly\n",
    "data_list     = load('csr_data_list')\n",
    "failed_urls   = load('failed_urls')\n",
    "finished_urls = load('finished_urls')\n",
    "\n",
    "# removes the URL number if that corresponding page has been scraped successfully\n",
    "for url in finished_urls:\n",
    "    try:\n",
    "        numbers_url.remove(url)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# tqdm() creates a progress bar to see how far you are from finishing your task\n",
    "# scrapes the website in a random order while cycling through IP addresses and user agents\n",
    "for number_url in tqdm(numbers_url):\n",
    "    try:\n",
    "        # being cautious\n",
    "        sleep(randint(30,60))\n",
    "        ua         = UserAgent()\n",
    "        user_agent = ua.random\n",
    "        headers    = {'User-Agent': user_agent}\n",
    "        tor = TorRequest(password = 'commonhorse')\n",
    "        tor.reset_identity()\n",
    "        \n",
    "        url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname='        + \\\n",
    "                '&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status='    + \\\n",
    "                '&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3'         + \\\n",
    "                '&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=' + \\\n",
    "                '&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=' + number_url\n",
    "\n",
    "\n",
    "        response = tor.get(url, headers = headers)\n",
    "\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        rows = html_soup.findAll('tr', {'title':'victim'})\n",
    "\n",
    "        # start scraping one page\n",
    "        for row in rows:\n",
    "            columns = row.findAll('td')\n",
    "\n",
    "            # saves the column data\n",
    "            for i in range(len(columns)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    data_list[i - 1].append(columns[i].text)\n",
    "                    \n",
    "        finished_urls.append(number_url)\n",
    "        \n",
    "        # stores the successfully scraped data\n",
    "        save(data_list    , 'csr_data_list')\n",
    "        # stores the finished URL number\n",
    "        save(finished_urls, 'finished_urls')\n",
    "    \n",
    "    # pages that failed are stored in the pickle file failed_urls\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('\\nFailed on: ', number_url)\n",
    "        failed_urls.append(number_url)\n",
    "        save(failed_urls, 'failed_urls')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're done with all the pages, we want to convert our saved pickle file into a CSV file (to make loading in the data easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a data frame\n",
    "victim_info = pd.DataFrame({\n",
    "    'victim_id'  : data_list[0],\n",
    "    'first_name' : data_list[1],\n",
    "    'father_name': data_list[2],\n",
    "    'last_name'  : data_list[3],\n",
    "    'province'   : data_list[4],\n",
    "    'town'       : data_list[5],\n",
    "    'date'       : data_list[6]    \n",
    "})\n",
    "\n",
    "# saves our file as a CSV file\n",
    "victim_info.to_csv(index=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
