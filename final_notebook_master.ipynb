{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casualties and Migration in the Syrian Civil War"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "***\n",
    "\n",
    "In 2011, five weeks into the civil demonstrations against the Syrian government, secret police forces detained and tortured fifteen students who had spray painted an anti-government statement on the walls of their school. They would be released weeks later in an effort to quell the rising civil unrest in the province. In the wake of the hundreds of other demonstrators who were killed or disappeared, this action was too little and too late to stop the tide of the civil war. Demonstrations turned to protest turned to armed conflict and the rest is history.\n",
    "\n",
    "The war would go on to spawn both the largest refugee crisis and one of the deadliest conflicts in modern history. As of 2019, there are over 6 million Syrian refugees and another 6 million internally displaced people in a country with a pre-war population of around 24 million (UNHCR, 2018). The regime's efforts to prevent accurate information from leaving the country has made it nearly impossible to estimate the number of casualties that have occured in that time. Current estimates range from 300,000 to 600,000 killed depending on the source.\n",
    "\n",
    "The link between the flow of violence within the country and the flow of asylum seekers out of the country should be apparent to anyone who is aware of the war. Yet a growing sentiment among residents in host countries is that a large portion of asylum seekers from Syria are actually economic migrants, who are using the conflict as a means of gaining entry into the European Union and access to generous social programs.\n",
    "\n",
    "We believe that violence is the most important predictor of migration of Syrian refugees; however, while this argument may be generally accepted, there is great difficulty in proving this relationship for certain. We hope to answer this question using reported casualty data to see whether there is a correlation between violence in a given province and a subsequent increase in the amount of asylum seekers across all host countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "***\n",
    "\n",
    "Our project can be organized into three distinct portions:\n",
    "\n",
    "1. Data Scraping\n",
    "2. Data Wrangling\n",
    "3. Data Visualization\n",
    "\n",
    "Our goal is to create a dataset for casualty information a refugee data, clean and structure the dataset for easier queying, and visualize the data to provide more insights into the questions we pose above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "***\n",
    "\n",
    "There are multiple sources that could be used for casualty information (list here). We will leave the three datasets for now, and focus on the VDC and CSR datasets because they provide their data is table elements that make it easy for us to scrape and organize our dataframes for analysis.\n",
    "\n",
    "We will now go through the process of scraping and creating the inital forms of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Violations Documentation Center](http://www.vdc-sy.info/) has been recording casualty data since June 2011. It is likely the most detailed and complete (in terms of metadata) data source of casualties that is publicly accessible.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `Name                  - Full name in English`\n",
    "- `Status                - Civilian, non-civilian, or military status of deceased`\n",
    "- `Sex                   - Whether deceased is an Adult or Minor and Male or Female`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Area \\ Place of Birth - Various locations that can be Provinces/Subdistricts/Towns`\n",
    "- `Date of death         - self explanatory`\n",
    "- `Cause of death        - self explanatory`\n",
    "- `Actors                - groups involved in the casualty`\n",
    "\n",
    "Each entry is associated with a unique identifier, which is an integer between 0 and 250,000. Clicking on the name of the entry will lead the user to another page that provides the unique identifier number and other data that is not displayed on the main page. We will avoid describing this detail for now, since most of this data is not used in the final product.\n",
    "\n",
    "We will describe the full process we used to scrape all details from this website as well as the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent():\n",
    "    first_page = 'http://www.vdc-sy.info/index.php/en/martyrs/1/c29ydGJ5PWEua2lsbGVkX2RhdGV8c29ydGRpcj1ERVNDfGFwcHJvdmVkPXZpc2libGV8ZXh0cmFkaXNwbGF5PTB8'\n",
    "    \n",
    "    # This is the format of the links that give us the unique identfiers\n",
    "    pattern    = re.compile('\\/index\\.php\\/en\\/details\\/martyrs\\/.')\n",
    "\n",
    "    # We want to establish a randomized user agent and Tor node to avoid detection\n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    tor        = TorRequest(password = 'commonhorse')\n",
    "    \n",
    "    try:\n",
    "        response = tor.get(first_page, headers=headers)\n",
    "        content  = bs(response.text, 'html.parser')\n",
    "        \n",
    "        # This list comprehension grabs all unique identifiers in string format for all links that match\n",
    "        # our regex pattern from above\n",
    "        links    = {link['href'][30:] for link in content.find_all('a', href = True) if pattern.match(link['href'])} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Provided a list of unique identifiers in string fromat, scrapes details and saves each entry \n",
    "as an idividual dataframe that represents one person.\n",
    "'''\n",
    "\n",
    "def scrape_details(uid, tor, headers):\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    url  = 'http://www.vdc-sy.info/index.php/en/details/martyrs/' + uid\n",
    "    \n",
    "    # Headers will provide the UserAgent to use when getting response\n",
    "    # Makes the request using a TorRequest object passed in\n",
    "    page = tor.get(url, headers = headers).text\n",
    "    page = bs(page, 'html.parser')\n",
    "    \n",
    "    # Grabs the relevant table info and all rows in it\n",
    "    table = page.find('table', attrs = {'class':'peopleListing'})\n",
    "    rows  = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "\n",
    "        # All data without only 2 data values\n",
    "        # are not data we are looking for\n",
    "        if len(data) != 2:\n",
    "            continue\n",
    "\n",
    "        # data[0] corresponds to the row label/column\n",
    "        cols.append(data[0].text)\n",
    "        \n",
    "        # Values need to appended differently for image rows \n",
    "        if data[1].find('img') is not None:\n",
    "            vals.append(data[1].find('img')['src'])\n",
    "        else:\n",
    "            vals.append(data[1].text)\n",
    "\n",
    "    # Adds the uid to the dataframe\n",
    "    cols.append('uid')\n",
    "    vals.append(uid)\n",
    "\n",
    "    # Creates and saves dataframe\n",
    "    person = pd.DataFrame([vals], columns = cols, dtype=str)\n",
    "\n",
    "    save(person, os.path.join('person_dfs', uid))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each detailed page has a different number of columns depending on the metadata associated with that entry, so we will now have to combine all the dataframes. Pandas requires that columns have unique names, so we have to rename all duplicate columns using this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dup_cols(dataframe):\n",
    "    cols = pd.Series(dataframe.columns)\n",
    "  \n",
    "    for dup in dataframe.columns.get_duplicates(): \n",
    "        cols[dataframe.columns.get_loc(dup)] = [dup + '_' + str(d_idx) if d_idx != 0 else dup for d_idx in range(dataframe.columns.get_loc(dup).sum())]\n",
    "   \n",
    "    dataframe.columns = cols\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given a list of dataframes we can return a combined dataframe that retains all column data and saves that file as vdc_df and saves any failed dataframes as failed_vdc_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes):\n",
    "    failed_dataframes = []\n",
    "    combined          = pd.DataFrame()\n",
    "\n",
    "    current = 0\n",
    "    num     = len(dataframes)\n",
    "\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            combined = pd.concat([combined, df], axis = 0)\n",
    "            print(f'{counter} / {num} people processed in combine_dataframes().')\n",
    "            counter += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_dataframes.append(df)\n",
    "            print('Failed')\n",
    "            counter += 1\n",
    "\n",
    "    save(combined, 'vdc_df')\n",
    "    save(failed_dataframes, 'failed_vdc_df')\n",
    "\n",
    "    print('\\n\\nSuccess: ', len(dataframes) - len(failed_dataframes))\n",
    "    print('Failed: ', len(failed_dataframes))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding this all together. We will now:\n",
    "\n",
    "1. Build a list of unique identifiers by scraping the query page for the VDC database using scrape_recent()\n",
    "\n",
    "2. Scrape the detailed information provided the list of unique ids from scrape_recent() using scrape_details, which gives us dataframes for each person.\n",
    "\n",
    "3. Combine those dataframes into one large dataset using combine_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_to_scrape = scrape_recent()\n",
    "uids_scraped   = set()\n",
    "\n",
    "while len(uids_to_scrape) > 0:\n",
    "    uid = uids_to_scrape.pop()\n",
    "    \n",
    "    try:\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        scrape_details(uid, tor, headers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        helen___uids_to_scrape.append(uid)\n",
    "\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        tor.reset_identity()\n",
    "\n",
    "        continue\n",
    "        \n",
    "    uids_scraped.add(uid)\n",
    "\n",
    "    save(uids_to_scrape, 'uids_to_scrape')\n",
    "    save(uids_scraped  , 'uids_scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "\n",
    "for person_df in glob.glob(os.path.join('person_dfs', '*.pickle')):\n",
    "    list_of_dataframes.append(load(person_df))\n",
    "    \n",
    "combine_dataframes(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Syrian Center for Statistics and Research](https://csr-sy.org/) has been recording casualty data since March 2011. It has less information than the VDC dataset, but the location of death is more precise.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `ID Number             - Arbitrary ID number`\n",
    "- `First Name            - First name in Arabic`\n",
    "- `Father Name           - Father's last name in Arabic`\n",
    "- `Last Name             - Last name in Arabic`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Town                  - Town where they died`\n",
    "- `Date of death         - self explanatory`\n",
    "\n",
    "If you looked at the code to scrape the VDC website, you'll see that there is no package for Tor or user agent. For some reason, this particular website was cautious about who was looking at their data as it blocked our multiple attempts of trying to scrape without cycling through IP addresses and user agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contain all the libraries that must be imported in order to run the web scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for requesting content of a web page\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "# for regex\n",
    "# import re\n",
    "\n",
    "# for dataframe creation\n",
    "import pandas as pd\n",
    "\n",
    "# for not overwhelming server when scraping pages\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "# For a progress bar while scraping\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For saving files\n",
    "import pickle\n",
    "\n",
    "# For shuffling list\n",
    "import random\n",
    "\n",
    "# For Tor Requests\n",
    "from torrequest     import TorRequest\n",
    "\n",
    "# To cycle through useragents\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not have to rescrape everything if one page fails, it is essential to have a pickle file that you can store your successfully scraped data and failed attempts in. Below is the code to create and load a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load functions for a pickle file\n",
    "def save(obj, name):\n",
    "    pickle.dump(obj, open(name + '.pickle', 'wb'))\n",
    "\n",
    "def load(name):\n",
    "    return pickle.load(open(name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url to start scraping CSR\n",
    "url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname=&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status=&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=0'\n",
    "\n",
    "response = get(url)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website was odd in the sense that some pages at the end didn't contain any information about Syrian casualties. We found the last page that had any information and hard-coded the page number in. As you can see from the code below, 91900 was the last page to contain any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of numbers used in shuffling through all the URLs\n",
    "numbers_url = [str(i) for i in range(0, 91901, 50)]\n",
    "\n",
    "# shuffling the numbers so that the website doesn't become suspicious of us\n",
    "random.shuffle(numbers_url)\n",
    "numbers_url = set(numbers_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the pickle files: the good, the bad, and the ugly\n",
    "data_list     = load('csr_data_list')\n",
    "failed_urls   = load('failed_urls')\n",
    "finished_urls = load('finished_urls')\n",
    "\n",
    "# removes the URL number if that corresponding page has been scraped successfully\n",
    "for url in finished_urls:\n",
    "    try:\n",
    "        numbers_url.remove(url)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# tqdm() creates a progress bar to see how far you are from finishing your task\n",
    "# scrapes the website in a random order while cycling through IP addresses and user agents\n",
    "for number_url in tqdm(numbers_url):\n",
    "    try:\n",
    "        # being cautious\n",
    "        sleep(randint(30,60))\n",
    "        ua         = UserAgent()\n",
    "        user_agent = ua.random\n",
    "        headers    = {'User-Agent': user_agent}\n",
    "        tor = TorRequest(password = 'commonhorse')\n",
    "        tor.reset_identity()\n",
    "        \n",
    "        url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname='        + \\\n",
    "                '&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status='    + \\\n",
    "                '&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3'         + \\\n",
    "                '&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=' + \\\n",
    "                '&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=' + number_url\n",
    "\n",
    "\n",
    "        response = tor.get(url, headers = headers)\n",
    "\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        rows = html_soup.findAll('tr', {'title':'victim'})\n",
    "\n",
    "        # start scraping one page\n",
    "        for row in rows:\n",
    "            columns = row.findAll('td')\n",
    "\n",
    "            # saves the column data\n",
    "            for i in range(len(columns)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    data_list[i - 1].append(columns[i].text)\n",
    "                    \n",
    "        finished_urls.append(number_url)\n",
    "        \n",
    "        # stores the successfully scraped data\n",
    "        save(data_list    , 'csr_data_list')\n",
    "        # stores the finished URL number\n",
    "        save(finished_urls, 'finished_urls')\n",
    "    \n",
    "    # pages that failed are stored in the pickle file failed_urls\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('\\nFailed on: ', number_url)\n",
    "        failed_urls.append(number_url)\n",
    "        save(failed_urls, 'failed_urls')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're done with all the pages, we want to convert our saved pickle file into a CSV file (to make loading in the data easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a data frame\n",
    "victim_info = pd.DataFrame({\n",
    "    'victim_id'  : data_list[0],\n",
    "    'first_name' : data_list[1],\n",
    "    'father_name': data_list[2],\n",
    "    'last_name'  : data_list[3],\n",
    "    'province'   : data_list[4],\n",
    "    'town'       : data_list[5],\n",
    "    'date'       : data_list[6]    \n",
    "})\n",
    "\n",
    "# saves our file as a CSV file\n",
    "victim_info.to_csv(index=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to see what the dataset looks like without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdc_df = load('vdc_df')\n",
    "vdc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the added details we got from scraping everythign from the website are valuable for more detailed analysis, these particular columns will be what we will be focusing on with this project:\n",
    "\n",
    "- `Name                  `\n",
    "- `Status                `\n",
    "- `Sex                   `\n",
    "- `Province              `\n",
    "- `Area \\ Place of Birth `\n",
    "- `Date of death         `\n",
    "- `Cause of death        `\n",
    "- `Actors                `\n",
    "\n",
    "And we can create a dataframe we will use to do all of their data frame so that we are not modifying the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = vdc_df[['Province',\n",
    "                  'Sex',\n",
    "                  'Status',\n",
    "                  'Date of death',\n",
    "                  'Cause of Death']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `Sex` column, we can see that there is actually data about the person's minority status and age range, so we will create new columns to capture that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first want to drop any rows that don't have this information\n",
    "scratch = scratch.dropna(subset=['Sex'])\n",
    "\n",
    "def check_age(row):\n",
    "    if 'Adult' in row['Sex']:\n",
    "        val = 'adult'\n",
    "    else:\n",
    "        val = 'minor'\n",
    "    return val\n",
    "\n",
    "scratch['age_cat'] = scratch.apply(check_age, axis=1)\n",
    "\n",
    "def check_sex(row):\n",
    "    if 'Male' in row['Sex']:\n",
    "        val = 'male'\n",
    "    else:\n",
    "        val = 'female'\n",
    "    return val\n",
    "\n",
    "scratch['sex'] = scratch.apply(check_sex, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `Cause of Death` coolumn, we'll see that there is some reduntant categories, so we'll simplify these categories by remapping those values based on a dictionary mapping we show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_map = {'Chemical and toxic gases'         : 'Chemical Weapon',\n",
    "                      'Detention - Execution'            : 'Detention',\n",
    "                      'Detention - Torture'              : 'Detention',\n",
    "                      'Detention - Torture - Execution'  : 'Detention',\n",
    "                      'Explosion'                        : 'Explosion',\n",
    "                      'Field Execution'                  : 'Execution',\n",
    "                      'Kidnapping - Execution'           : 'Execution',\n",
    "                      'Kidnapping - Torture'             : 'Execution',\n",
    "                      'Kidnapping - Torture - Execution' : 'Execution',\n",
    "                      'Other'                            : 'Unknown'  ,\n",
    "                      'Shelling'                         : 'Shelling' ,\n",
    "                      'Shooting'                         : 'Shooting' ,\n",
    "                      'Siege'                            : 'Siege'    ,\n",
    "                      'Un-allowed to seek Medical help'  : 'Lack of Medical Access',\n",
    "                      'Unknown'                          : 'Unknown'  ,\n",
    "                      'Warplane shelling'                : 'Shelling' \n",
    "}\n",
    "\n",
    "def check_cause_of_death(row, mapping):\n",
    "    return mapping[row['Cause of Death']]\n",
    "\n",
    "scratch['cause_of_death'] = scratch.apply(check_cause_of_death,\n",
    "                                        args = (cause_of_death_map, ),\n",
    "                                        axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we can change the status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(row):\n",
    "    if row['Status'] == 'Non-Civilian':\n",
    "        val = 'non_civilian'\n",
    "    elif row['Status'] == 'Civilian':\n",
    "        val = 'civilian'\n",
    "    else:\n",
    "        val = 'regime'\n",
    "    return val\n",
    "\n",
    "scratch['status'] = scratch.apply(check_status, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is cleaner, we can drop columns that irrelevant to us, and rename the columns for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = scratch[['Province',\n",
    "                  'sex',\n",
    "                  'status',\n",
    "                  'age_cat',\n",
    "                  'Date of death',\n",
    "                  'cause_of_death']].copy()\n",
    "\n",
    "scratch.columns = ['province', 'sex', 'status', 'age_cat','date_of_death', 'cause_of_death']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now drop any entries with unrecroded or icorrect dates of death and convert the time strings to python datetime objects.\n",
    "\n",
    "With all of those modifcations we can finally save this dataset as complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = scratch[scratch['province'].isin(picked)]\n",
    "scratch = scratch[scratch['date_of_death'] != '0000-00-00']\n",
    "scratch = scratch[scratch['date_of_death'] != '1970-01-01']\n",
    "scratch['date_of_death'] = pd.to_datetime(scratch['date_of_death'])\n",
    "\n",
    "save(scratch, 'clean_vdc_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year-by-Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# call this so that running plot_bokeh won't create \n",
    "# a new window and results will be shown in notebook\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is provincial for VDC, we decided to use the provincial shapefile to map the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the shape file and save it as a geo data frame\n",
    "shp_file = os.path.join('syr_admin_shp_utf8_18219', 'syr_admin1.shp')\n",
    "map_df   = gpd.read_file(shp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the VDC csv file and save it as a pandas data frame\n",
    "dataset = pd.read_csv('vdc_data.csv', encoding='latin-1', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VDC data set's \"Date of death\" column contains the month, day, and year the person died, but we only wanted to see the yearly fluctuations. Therefore, we took a substring of the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data has month and day, so we took a substring of the year of death\n",
    "dataset['Year of Death'] = dataset['Date of death'].str[:4]\n",
    "\n",
    "# counts the number of times a province is in the dataset for a certain year\n",
    "province_count = dataset.groupby(['Province', 'Year of Death']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns to make data frame smaller\n",
    "simplified_df = province_count.drop(province_count.columns[1:], axis=1)\n",
    "simplified_df = simplified_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns to make data frame smaller\n",
    "simplified_df = province_count.drop(province_count.columns[1:], axis=1)\n",
    "simplified_df = simplified_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slider for Bokeh maps works with column titles, not row values, so we had to pivot the table. The once \"Date of death\" values became column titles and the count of Syrian casualties for that particular year and country became the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so years are columns rather than values\n",
    "year_as_column = simplified_df.pivot_table('Unnamed: 0', 'Province', 'Year of Death')\n",
    "year_as_column.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant years\n",
    "year_as_column = year_as_column.drop(['0000', '1970'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the province names in the VDC data set and the shapefile for Syria were not the same, so we had to go through the names manually and see which ones were different so that we could match the different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing province names by hand\n",
    "name_change = {\n",
    "    'Damascus Suburbs': 'Rural Damascus',\n",
    "    'Daraa': 'Dar\\'a',\n",
    "    'Deir Ezzor': 'Deir-ez-Zor',\n",
    "    'Hasakeh': 'Al-Hasakeh',\n",
    "    'Idlib': 'Idleb',\n",
    "    'Raqqa': 'Ar-Raqqa',\n",
    "    'Sweida': 'As-Sweida'\n",
    "}\n",
    "\n",
    "# renames the provinces using name_change\n",
    "year_as_column.replace(name_change, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining data from casualties (VDC) and geo data frame (shape file)\n",
    "merged = year_as_column.set_index('Province').join(map_df.set_index('NAME_EN'))\n",
    "merged.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant information\n",
    "# row where there were no data for geo data\n",
    "merged = merged.drop([10, 15], axis=0)\n",
    "# columns with information not pertaining to creating choropleth map\n",
    "merged.drop(merged.columns[9:16], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with plot_bokeh() is that it only takes immutable objects. However, the Pandas dataframe is mutable. Therefore, in order to bypass this problem, we decided to convert the Pandas dataframe into a GeoDataFrame. The following code describes that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe to GeoDataFrame\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = merged['geometry']\n",
    "merged_gdf = merged.drop(['geometry'], axis=1)\n",
    "crs = {'init': 'epsg:4326'}\n",
    "gdf = GeoDataFrame(merged_gdf, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify slider columns:\n",
    "slider_columns = [\"201%d\"%i for i in range(1, 8)]\n",
    "slider_range = range(2011, 2018)\n",
    "\n",
    "# make slider plot:\n",
    "gdf.plot_bokeh(\n",
    "    figsize=(900, 600),\n",
    "    slider=slider_columns,\n",
    "    slider_range=slider_range,\n",
    "    slider_name=\"Year\", \n",
    "    colormap='Inferno',\n",
    "    hovertool_columns=[\"Province\"],\n",
    "    title=\"Deaths in Syria\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year-by-Year with plotly and mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle and read the file\n",
    "import pickle\n",
    "final = pickle.load(open('./death_by_province_by_year.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://plot.ly/python/scattermapbox/\n",
    "# reference: https://community.periscopedata.com/t/36nz2s/plotly-choropleth-with-slider-map-charts-over-time\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# this is a public mapbox token\n",
    "mapbox_access_token = 'pk.eyJ1IjoibWF0dGhld2lydHoiLCJhIjoiY2p3ZTNpNXlnMHYxcjQ5bzdwMjc0anlpeSJ9.bSLA-SSqEomk0hC52rNliQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MattWirtz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first year\n",
    "year = 2011\n",
    "\n",
    "# for every year we need the latitude, longitude, and the # of casualties\n",
    "# we use a for loop to store the data for each year of the Syrian War\n",
    "data_slider = []\n",
    "for year in final['year'].unique():\n",
    "    sect =  final[(final['year']== year)]\n",
    "\n",
    "    for col in sect.columns:\n",
    "        sect[col] = sect[col].astype(str)\n",
    "        \n",
    "    data_each_yr = go.Scattermapbox(\n",
    "        name=str(year),\n",
    "        lat=['36.2021',\n",
    "             '33.5138',\n",
    "             '33.5167',\n",
    "             '32.6264',\n",
    "             '35.3297',\n",
    "             '35.1409',\n",
    "             '36.5079',\n",
    "             '34.7324',\n",
    "             '35.9310',\n",
    "             '33.1219',\n",
    "             '35.5407',  \n",
    "             '35.9594',  \n",
    "             '32.7129', \n",
    "             '34.8959'],\n",
    "        lon=['37.1343',\n",
    "             '36.2765',\n",
    "             '36.9541',\n",
    "             '36.1033',\n",
    "             '40.1350',\n",
    "             '36.7552',\n",
    "             '40.7463',\n",
    "             '36.7137',\n",
    "             '36.6418',\n",
    "             '35.8209',\n",
    "             '35.7953', \n",
    "             '38.9981',  \n",
    "             '36.5663',\n",
    "             '35.8867'],\n",
    "        text=[\"Aleppo\",\n",
    "              \"Damascus\",\n",
    "              \"Rural Damascus\",\n",
    "              \"Daraa\",\n",
    "              \"Deir Ezzor\",\n",
    "              \"Hama\",\n",
    "              \"Hasakeh\",\n",
    "              \"Homs\",\n",
    "              \"Idlib\",\n",
    "              \"Lattakia\",\n",
    "              \"Quneitra\", \n",
    "              \"Ar Raqqah\", \n",
    "              \"As Suwayda\",\n",
    "              \"Tartus\"],\n",
    "        mode='markers',\n",
    "        marker = go.scattermapbox.Marker(\n",
    "            size = sect['casualties'].astype(int),\n",
    "            color = 'rgb(255, 0, 0)',\n",
    "            sizemode = 'area',\n",
    "        ), visible=False\n",
    "    )\n",
    "    data_slider.append(data_each_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the steps are for each of the possible places our slider can rest\n",
    "steps = []\n",
    "for i in range(len(data_slider)):\n",
    "    step = dict(method='restyle',\n",
    "                args=['visible', [False] * len(data_slider)],\n",
    "                label='Year {}'.format(i + 2011))\n",
    "    step['args'][1][i] = True\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=0, pad={\"t\": 1}, steps=steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up our frame with the mapbox\n",
    "layout = go.Layout(\n",
    "    showlegend=False,\n",
    "    title = go.layout.Title(\n",
    "            text = '2011-2018 Syrian Casualties'\n",
    "        ),\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    sliders=sliders,\n",
    "    mapbox=go.layout.Mapbox(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=go.layout.mapbox.Center(\n",
    "            lat=34.7,\n",
    "            lon=37.2\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=5.4\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ksuhr1/25.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize\n",
    "fig = go.Figure(data=data_slider, layout=layout)\n",
    "py.iplot(fig, filename='Multiple-Mapbox.html')\n",
    "# plot(fig, filename='Multiple-Mapbox.html')\n",
    "# use ^ to have the graph open into a seperate tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day-by-Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>Day-by-Day</b> code is similar to the <b>Year-by-Year</b> code, so we hope the viewer doesn't mind if we have less comments in this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the shape file and save it as a geo data frame\n",
    "shp_file = os.path.join('syr_admin_shp_utf8_18219', 'syr_admin1.shp')\n",
    "map_df   = gpd.read_file(shp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle objects as a pandas data frame\n",
    "day_df = pickle.load(open('./death_by_province_by_day.pickle', 'rb'))\n",
    "\n",
    "# read the VDC csv file specifically for column \"geometry\"\n",
    "dataset = pd.read_csv('vdc_data.csv', encoding='latin-1', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing province names by hand\n",
    "name_change = {\n",
    "    'Damascus Suburbs': 'Rural Damascus',\n",
    "    'Daraa': 'Dar\\'a',\n",
    "    'Deir Ezzor': 'Deir-ez-Zor',\n",
    "    'Raqqa': 'Ar-Raqqa',\n",
    "    'Sweida': 'As-Sweida',\n",
    "    'Idlib': 'Idleb',\n",
    "    'Hasakeh': 'Al-Hasakeh',\n",
    "}\n",
    "\n",
    "# renames the provinces using name_change\n",
    "day_df.replace(name_change, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so days are columns rather than values\n",
    "pivoted_df = day_df.pivot_table('casualties','province','day').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the data frames in order to obtain the geo data\n",
    "use = pivoted_df.join(map_df.set_index('NAME_EN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any unnecessary columns\n",
    "ready = use.drop(columns=['NAM_EN_REF','NAME_AR','PCODE','ADM0_EN','ADM0_AR','ADM0_PCODE','UPDATE_DAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of days from the first date of death to the last date of death is 2,686 days. Therefore, we created 2,686 columns for each day. (Note: this may take a while to finish running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"year-month-day time\" and replace it with the \"day\"\n",
    "for i in range(0, 2687):\n",
    "    ready = ready.rename(index=str, columns={ready.columns[i]: str(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe to GeoDataFrame\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = ready['geometry']\n",
    "crs = {'init': 'epsg:4326'}\n",
    "day_gdf = GeoDataFrame(ready, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 'province' a column\n",
    "day_gdf = day_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with our visualization is that most of the deaths are in the single to double digits. Very few times do the number of deaths on a single day go into the triple digits. However, the color bar is uniformly split, and we weren't able to find documentation on how to split up the bokeh color bar up, so we brute forced that colors in the color bar.<br><br>\n",
    "What we wanted to display was that between 0 and 100, the difference in hue would be greater than the difference in hue between 101-600. Therefore, there would be a bigger variety of colors on the map at a certain time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify slider columns\n",
    "slider_columns = []\n",
    "for i in range (0, 2687):\n",
    "    slider_columns.append(str(i))\n",
    "\n",
    "slider_range = range(0, 2687)\n",
    "\n",
    "# make slider plot\n",
    "day_gdf.plot_bokeh(\n",
    "    figsize=(900, 600),\n",
    "    slider=slider_columns,\n",
    "    slider_range=slider_range,\n",
    "    slider_name=\"Day\",\n",
    "    # brute force color bar for map\n",
    "    colormap=['#edf8f3', '#dcf2e8', '#cbebdd', '#b9e5d2', '#a8dfc7', '#97d8bc', '#85d2b1', '#74cba6', '#63c59b', '#52bf90',\n",
    "              '#52bf90', '#49ab81', '#419873', '#398564', '#317256', '#295f48', '#204c39', '#18392b', '#18392b', '#18392b', \n",
    "              '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', \n",
    "              '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812',\n",
    "              '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', \n",
    "              '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "    ],\n",
    "    hovertool_columns=[\"province\"],\n",
    "    title=\"Deaths in Syria\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
