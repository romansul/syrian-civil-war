{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casualties and Migration in the Syrian Civil War"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "***\n",
    "\n",
    "In 2011, five weeks into the civil demonstrations against the Syrian government, secret police forces detained and tortured fifteen students who had spray painted an anti-government statement on the walls of their school. They would be released weeks later in an effort to quell the rising civil unrest in the province. In the wake of the hundreds of other demonstrators who were killed or disappeared, this action was too little and too late to stop the tide of the civil war. Demonstrations turned to protest turned to armed conflict and the rest is history.\n",
    "\n",
    "The war would go on to spawn both the largest refugee crisis and one of the deadliest conflicts in modern history. As of 2019, there are over 6 million Syrian refugees and another 6 million internally displaced people in a country with a pre-war population of around 24 million (UNHCR, 2018). The regime's efforts to prevent accurate information from leaving the country has made it nearly impossible to estimate the number of casualties that have occured in that time. Current estimates range from 300,000 to 600,000 killed depending on the source.\n",
    "\n",
    "The link between the flow of violence within the country and the flow of asylum seekers out of the country should be apparent to anyone who is aware of the war. Yet a growing sentiment among residents in host countries is that a large portion of asylum seekers from Syria are actually economic migrants, who are using the conflict as a means of gaining entry into the European Union and access to generous social programs.\n",
    "\n",
    "We believe that violence is the most important predictor of migration of Syrian refugees; however, while this argument may be generally accepted, there is great difficulty in proving this relationship for certain. We hope to answer this question using reported casualty data to see whether there is a correlation between violence in a given province and a subsequent increase in the amount of asylum seekers across all host countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "***\n",
    "\n",
    "Our project can be organized into three distinct portions:\n",
    "\n",
    "1. Data Scraping\n",
    "2. Data Wrangling\n",
    "3. Data Visualization\n",
    "\n",
    "Our goal is to create a dataset for casualty information a refugee data, clean and structure the dataset for easier queying, and visualize the data to provide more insights into the questions we pose above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "***\n",
    "\n",
    "There are multiple sources that could be used for casualty information (list here). We will leave the three datasets for now, and focus on the VDC and CSR datasets because they provide their data is table elements that make it easy for us to scrape and organize our dataframes for analysis.\n",
    "\n",
    "We will now go through the process of scraping and creating the inital forms of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Violations Documentation Center](http://www.vdc-sy.info/) has been recording casualty data since June 2011. It is likely the most detailed and complete (in terms of metadata) data source of casualties that is publicly accessible.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `Name                  - Full name in English`\n",
    "- `Status                - Civilian, non-civilian, or military status of deceased`\n",
    "- `Sex                   - Whether deceased is an Adult or Minor and Male or Female`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Area \\ Place of Birth - Various locations that can be Provinces/Subdistricts/Towns`\n",
    "- `Date of death         - self explanatory`\n",
    "- `Cause of death        - self explanatory`\n",
    "- `Actors                - groups involved in the casualty`\n",
    "\n",
    "Each entry is associated with a unique identifier, which is an integer between 0 and 250,000. Clicking on the name of the entry will lead the user to another page that provides the unique identifier number and other data that is not displayed on the main page. We will avoid describing this detail for now, since most of this data is not used in the final product.\n",
    "\n",
    "We will describe the full process we used to scrape all details from this website as well as the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent():\n",
    "    first_page = 'http://www.vdc-sy.info/index.php/en/martyrs/1/c29ydGJ5PWEua2lsbGVkX2RhdGV8c29ydGRpcj1ERVNDfGFwcHJvdmVkPXZpc2libGV8ZXh0cmFkaXNwbGF5PTB8'\n",
    "    \n",
    "    # This is the format of the links that give us the unique identfiers\n",
    "    pattern    = re.compile('\\/index\\.php\\/en\\/details\\/martyrs\\/.')\n",
    "\n",
    "    # We want to establish a randomized user agent and Tor node to avoid detection\n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    tor        = TorRequest(password = 'commonhorse')\n",
    "    \n",
    "    try:\n",
    "        response = tor.get(first_page, headers=headers)\n",
    "        content  = bs(response.text, 'html.parser')\n",
    "        \n",
    "        # This list comprehension grabs all unique identifiers in string format for all links that match\n",
    "        # our regex pattern from above\n",
    "        links    = {link['href'][30:] for link in content.find_all('a', href = True) if pattern.match(link['href'])} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Provided a list of unique identifiers in string fromat, scrapes details and saves each entry \n",
    "as an idividual dataframe that represents one person.\n",
    "'''\n",
    "\n",
    "def scrape_details(uid, tor, headers):\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    url  = 'http://www.vdc-sy.info/index.php/en/details/martyrs/' + uid\n",
    "    \n",
    "    # Headers will provide the UserAgent to use when getting response\n",
    "    # Makes the request using a TorRequest object passed in\n",
    "    page = tor.get(url, headers = headers).text\n",
    "    page = bs(page, 'html.parser')\n",
    "    \n",
    "    # Grabs the relevant table info and all rows in it\n",
    "    table = page.find('table', attrs = {'class':'peopleListing'})\n",
    "    rows  = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "\n",
    "        # All data without only 2 data values\n",
    "        # are not data we are looking for\n",
    "        if len(data) != 2:\n",
    "            continue\n",
    "\n",
    "        # data[0] corresponds to the row label/column\n",
    "        cols.append(data[0].text)\n",
    "        \n",
    "        # Values need to appended differently for image rows \n",
    "        if data[1].find('img') is not None:\n",
    "            vals.append(data[1].find('img')['src'])\n",
    "        else:\n",
    "            vals.append(data[1].text)\n",
    "\n",
    "    # Adds the uid to the dataframe\n",
    "    cols.append('uid')\n",
    "    vals.append(uid)\n",
    "\n",
    "    # Creates and saves dataframe\n",
    "    person = pd.DataFrame([vals], columns = cols, dtype=str)\n",
    "\n",
    "    save(person, os.path.join('person_dfs', uid))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each detailed page has a different number of columns depending on the metadata associated with that entry, so we will now have to combine all the dataframes. Pandas requires that columns have unique names, so we have to rename all duplicate columns using this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dup_cols(dataframe):\n",
    "    cols = pd.Series(dataframe.columns)\n",
    "  \n",
    "    for dup in dataframe.columns.get_duplicates(): \n",
    "        cols[dataframe.columns.get_loc(dup)] = [dup + '_' + str(d_idx) if d_idx != 0 else dup for d_idx in range(dataframe.columns.get_loc(dup).sum())]\n",
    "   \n",
    "    dataframe.columns = cols\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given a list of dataframes we can return a combined dataframe that retains all column data and saves that file as vdc_df and saves any failed dataframes as failed_vdc_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes):\n",
    "    failed_dataframes = []\n",
    "    combined          = pd.DataFrame()\n",
    "\n",
    "    current = 0\n",
    "    num     = len(dataframes)\n",
    "\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            combined = pd.concat([combined, df], axis = 0)\n",
    "            print(f'{counter} / {num} people processed in combine_dataframes().')\n",
    "            counter += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_dataframes.append(df)\n",
    "            print('Failed')\n",
    "            counter += 1\n",
    "\n",
    "    save(combined, 'vdc_df')\n",
    "    save(failed_dataframes, 'failed_vdc_df')\n",
    "\n",
    "    print('\\n\\nSuccess: ', len(dataframes) - len(failed_dataframes))\n",
    "    print('Failed: ', len(failed_dataframes))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding this all together. We will now:\n",
    "\n",
    "1. Build a list of unique identifiers by scraping the query page for the VDC database using scrape_recent()\n",
    "\n",
    "2. Scrape the detailed information provided the list of unique ids from scrape_recent() using scrape_details, which gives us dataframes for each person.\n",
    "\n",
    "3. Combine those dataframes into one large dataset using combine_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UserAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-22e503b99fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muids_to_scrape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_recent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muids_scraped\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muids_to_scrape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muids_to_scrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-270-f2fc6cf2255c>\u001b[0m in \u001b[0;36mscrape_recent\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# We want to establish a randomized user agent and Tor node to avoid detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mua\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mUserAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mheaders\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'User-Agent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtor\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mTorRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'commonhorse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UserAgent' is not defined"
     ]
    }
   ],
   "source": [
    "uids_to_scrape = scrape_recent()\n",
    "uids_scraped   = set()\n",
    "\n",
    "while len(uids_to_scrape) > 0:\n",
    "    uid = uids_to_scrape.pop()\n",
    "    \n",
    "    try:\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        scrape_details(uid, tor, headers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        helen___uids_to_scrape.append(uid)\n",
    "\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        tor.reset_identity()\n",
    "\n",
    "        continue\n",
    "        \n",
    "    uids_scraped.add(uid)\n",
    "\n",
    "    save(uids_to_scrape, 'uids_to_scrape')\n",
    "    save(uids_scraped  , 'uids_scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "\n",
    "for person_df in glob.glob(os.path.join('person_dfs', '*.pickle')):\n",
    "    list_of_dataframes.append(load(person_df))\n",
    "    \n",
    "combine_dataframes(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Syrian Center for Statistics and Research](https://csr-sy.org/) has been recording casualty data since March 2011. It has less information than the VDC dataset, but the location of death is more precise.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `ID Number             - Arbitrary ID number`\n",
    "- `First Name            - First name in Arabic`\n",
    "- `Father Name           - Father's last name in Arabic`\n",
    "- `Last Name             - Last name in Arabic`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Town                  - Town where they died`\n",
    "- `Date of death         - self explanatory`\n",
    "\n",
    "If you looked at the code to scrape the VDC website, you'll see that there is no package for Tor or user agent. For some reason, this particular website was cautious about who was looking at their data as it blocked our multiple attempts of trying to scrape without cycling through IP addresses and user agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contain all the libraries that must be imported in order to run the web scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for requesting content of a web page\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "# for regex\n",
    "# import re\n",
    "\n",
    "# for dataframe creation\n",
    "import pandas as pd\n",
    "\n",
    "# for not overwhelming server when scraping pages\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "# For a progress bar while scraping\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For saving files\n",
    "import pickle\n",
    "\n",
    "# For shuffling list\n",
    "import random\n",
    "\n",
    "# For Tor Requests\n",
    "from torrequest     import TorRequest\n",
    "\n",
    "# To cycle through useragents\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not have to rescrape everything if one page fails, it is essential to have a pickle file that you can store your successfully scraped data and failed attempts in. Below is the code to create and load a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load functions for a pickle file\n",
    "def save(obj, name):\n",
    "    pickle.dump(obj, open(name + '.pickle', 'wb'))\n",
    "\n",
    "def load(name):\n",
    "    return pickle.load(open(name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url to start scraping CSR\n",
    "url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname=&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status=&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=0'\n",
    "\n",
    "response = get(url)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website was odd in the sense that some pages at the end didn't contain any information about Syrian casualties. We found the last page that had any information and hard-coded the page number in. As you can see from the code below, 91900 was the last page to contain any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of numbers used in shuffling through all the URLs\n",
    "numbers_url = [str(i) for i in range(0, 91901, 50)]\n",
    "\n",
    "# shuffling the numbers so that the website doesn't become suspicious of us\n",
    "random.shuffle(numbers_url)\n",
    "numbers_url = set(numbers_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the pickle files: the good, the bad, and the ugly\n",
    "data_list     = load('csr_data_list')\n",
    "failed_urls   = load('failed_urls')\n",
    "finished_urls = load('finished_urls')\n",
    "\n",
    "# removes the URL number if that corresponding page has been scraped successfully\n",
    "for url in finished_urls:\n",
    "    try:\n",
    "        numbers_url.remove(url)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# tqdm() creates a progress bar to see how far you are from finishing your task\n",
    "# scrapes the website in a random order while cycling through IP addresses and user agents\n",
    "for number_url in tqdm(numbers_url):\n",
    "    try:\n",
    "        # being cautious\n",
    "        sleep(randint(30,60))\n",
    "        ua         = UserAgent()\n",
    "        user_agent = ua.random\n",
    "        headers    = {'User-Agent': user_agent}\n",
    "        tor = TorRequest(password = 'commonhorse')\n",
    "        tor.reset_identity()\n",
    "        \n",
    "        url = 'https://csr-sy.org/?l=1&sons=redirect&sequence=&name=&father_name=&surname='        + \\\n",
    "                '&age_from=0&age_to=120&gender=&born_state=&born_town=&career=&society_status='    + \\\n",
    "                '&sons_no=&medical_status=&incident_state=&incident_town=&incident_desc=3'         + \\\n",
    "                '&incident_date_from=&incident_date_to=&incident_details=&trial=&trial_date_from=' + \\\n",
    "                '&trial_date_to=&id=182&ddate_from=&ddate_to=&rec=' + number_url\n",
    "\n",
    "\n",
    "        response = tor.get(url, headers = headers)\n",
    "\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        rows = html_soup.findAll('tr', {'title':'victim'})\n",
    "\n",
    "        # start scraping one page\n",
    "        for row in rows:\n",
    "            columns = row.findAll('td')\n",
    "\n",
    "            # saves the column data\n",
    "            for i in range(len(columns)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    data_list[i - 1].append(columns[i].text)\n",
    "                    \n",
    "        finished_urls.append(number_url)\n",
    "        \n",
    "        # stores the successfully scraped data\n",
    "        save(data_list    , 'csr_data_list')\n",
    "        # stores the finished URL number\n",
    "        save(finished_urls, 'finished_urls')\n",
    "    \n",
    "    # pages that failed are stored in the pickle file failed_urls\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('\\nFailed on: ', number_url)\n",
    "        failed_urls.append(number_url)\n",
    "        save(failed_urls, 'failed_urls')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're done with all the pages, we want to convert our saved pickle file into a CSV file (to make loading in the data easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a data frame\n",
    "victim_info = pd.DataFrame({\n",
    "    'victim_id'  : data_list[0],\n",
    "    'first_name' : data_list[1],\n",
    "    'father_name': data_list[2],\n",
    "    'last_name'  : data_list[3],\n",
    "    'province'   : data_list[4],\n",
    "    'town'       : data_list[5],\n",
    "    'date'       : data_list[6]    \n",
    "})\n",
    "\n",
    "# saves our file as a CSV file\n",
    "victim_info.to_csv(index=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [UN Refugee Agency [UNHCR]](http://popstats.unhcr.org/en/asylum_seekers_monthly) provides monthly refugee data in csv format with the following attributes:\n",
    "\n",
    "- `Years                 `\n",
    "- `Months                `\n",
    "- `Country / territory of asylum/residence                   `\n",
    "- `Origin  `\n",
    "\n",
    "The monthly data is from 1999-2018 and there are 38 European and 6 non-European countries provided about asylum applications. We selected data from the years 2011 - 2018 to focus on the the time frame of the Syrian Civil War which stared in March 15, 2011. You can download the csv file by selecting the following categories:\n",
    "- `Years: 2011-2018                 `\n",
    "- `Months: All months               `\n",
    "- `Country / territory of asylum/residence: All countries/territories                   `\n",
    "- `Origin: Syrian Arab Republic `\n",
    "\n",
    "The csv file is readily available so we did not have to do any further scraping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries used for Refugee Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "plotly.__version__\n",
    "init_notebook_mode(connected=True)\n",
    "plotly.tools.set_credentials_file(username='katy27', api_key='gnvKtGJNSZTspVTRINvI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file into pandas dataframe\n",
    "refugee_month_df = pd.read_csv('2011-2018_monthly_refugee.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [UN Refugee Agency [UNHCR]](https://data.world/unhcr/e2d5566d-d755-40dd-a63f-d4d298a9df1d/workspace/file?filename=unhcr-persons-of-concern-origin-syr-csv-1.csv) provides year-by-year data from 1968-2017 concerning refugee migrations and more population concerns originating from Syria. We can can access this data in csv format easily, however the link provided may ask the user to sign in with a dataworld account or  google/facebook profile. The dataset contains the following attributes:\n",
    "\n",
    "- `Year                 `\n",
    "- `Country / territory of asylum/residence                `\n",
    "- `Origin                   `\n",
    "- `Refugees (incl. refugee-like situations  `\n",
    "- `Asylum-seekers (pending cases)                 `\n",
    "- `Returned refugees               `\n",
    "- `Internally displaced persons (IDPs)                `\n",
    "- `Returned IDPs  `\n",
    "- `Stateless persons                 `\n",
    "- `Total Population `\n",
    "\n",
    "We are interested in selecting the following columns and filter our data frame accordingly:\n",
    "- `Year                 `\n",
    "- `Country / territory of asylum/residence                `\n",
    "- `Origin                   `\n",
    "- `Refugees (incl. refugee-like situations  `\n",
    "- `Asylum-seekers (pending cases)                 `\n",
    "- `Total Population `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file into pandas dataframe\n",
    "refugee_year_df = pd.read_csv('unhcr-persons-of-concern-origin-syr-csv-1.csv')\n",
    "\n",
    "# sort dataframe by ascending years\n",
    "refugee_year_df = refugee_year_df.sort_values('Year', ascending=True)\n",
    "\n",
    "# adjust index of dataframe\n",
    "refugee_year_df = refugee_year_df.reset_index()\n",
    "refugee_year_df = refugee_year_df.drop(refugee_df.index[0])\n",
    "refugee_year_df = refugee_year_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the dataset by selecting specific columns as well as time between 2011 and 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the years we want\n",
    "years = ['2011', '2012', '2013', '2014', '2015','2016', '2017']\n",
    "\n",
    "refugee_subset = refugee_year_df[refugee_year_df['Year'].isin(years)]\n",
    "\n",
    "# reset index\n",
    "refugee_subset = refugee_subset.reset_index()\n",
    "\n",
    "# drop index column\n",
    "bad_cols = ['index','Origin','Returned refugees','Returned refugees',\n",
    "            'Internally displaced persons (IDPs)','Returned IDPs',\n",
    "            'Stateless persons','Others of concern']\n",
    "refugee_subset = refugee_subset.drop(bad_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to see what the dataset looks like without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-43527589066f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvdc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vdc_df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvdc_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "vdc_df = load('vdc_df')\n",
    "vdc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the added details we got from scraping everythign from the website are valuable for more detailed analysis, these particular columns will be what we will be focusing on with this project:\n",
    "\n",
    "- `Name                  `\n",
    "- `Status                `\n",
    "- `Sex                   `\n",
    "- `Province              `\n",
    "- `Area \\ Place of Birth `\n",
    "- `Date of death         `\n",
    "- `Cause of death        `\n",
    "- `Actors                `\n",
    "\n",
    "And we can create a dataframe we will use to do all of their data frame so that we are not modifying the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = vdc_df[['Province',\n",
    "                  'Sex',\n",
    "                  'Status',\n",
    "                  'Date of death',\n",
    "                  'Cause of Death']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `Sex` column, we can see that there is actually data about the person's minority status and age range, so we will create new columns to capture that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first want to drop any rows that don't have this information\n",
    "scratch = scratch.dropna(subset=['Sex'])\n",
    "\n",
    "def check_age(row):\n",
    "    if 'Adult' in row['Sex']:\n",
    "        val = 'adult'\n",
    "    else:\n",
    "        val = 'minor'\n",
    "    return val\n",
    "\n",
    "scratch['age_cat'] = scratch.apply(check_age, axis=1)\n",
    "\n",
    "def check_sex(row):\n",
    "    if 'Male' in row['Sex']:\n",
    "        val = 'male'\n",
    "    else:\n",
    "        val = 'female'\n",
    "    return val\n",
    "\n",
    "scratch['sex'] = scratch.apply(check_sex, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `Cause of Death` coolumn, we'll see that there is some reduntant categories, so we'll simplify these categories by remapping those values based on a dictionary mapping we show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_map = {'Chemical and toxic gases'         : 'Chemical Weapon',\n",
    "                      'Detention - Execution'            : 'Detention',\n",
    "                      'Detention - Torture'              : 'Detention',\n",
    "                      'Detention - Torture - Execution'  : 'Detention',\n",
    "                      'Explosion'                        : 'Explosion',\n",
    "                      'Field Execution'                  : 'Execution',\n",
    "                      'Kidnapping - Execution'           : 'Execution',\n",
    "                      'Kidnapping - Torture'             : 'Execution',\n",
    "                      'Kidnapping - Torture - Execution' : 'Execution',\n",
    "                      'Other'                            : 'Unknown'  ,\n",
    "                      'Shelling'                         : 'Shelling' ,\n",
    "                      'Shooting'                         : 'Shooting' ,\n",
    "                      'Siege'                            : 'Siege'    ,\n",
    "                      'Un-allowed to seek Medical help'  : 'Lack of Medical Access',\n",
    "                      'Unknown'                          : 'Unknown'  ,\n",
    "                      'Warplane shelling'                : 'Shelling' \n",
    "}\n",
    "\n",
    "def check_cause_of_death(row, mapping):\n",
    "    return mapping[row['Cause of Death']]\n",
    "\n",
    "scratch['cause_of_death'] = scratch.apply(check_cause_of_death,\n",
    "                                        args = (cause_of_death_map, ),\n",
    "                                        axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we can change the status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(row):\n",
    "    if row['Status'] == 'Non-Civilian':\n",
    "        val = 'non_civilian'\n",
    "    elif row['Status'] == 'Civilian':\n",
    "        val = 'civilian'\n",
    "    else:\n",
    "        val = 'regime'\n",
    "    return val\n",
    "\n",
    "scratch['status'] = scratch.apply(check_status, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is cleaner, we can drop columns that irrelevant to us, and rename the columns for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = scratch[['Province',\n",
    "                  'sex',\n",
    "                  'status',\n",
    "                  'age_cat',\n",
    "                  'Date of death',\n",
    "                  'cause_of_death']].copy()\n",
    "\n",
    "scratch.columns = ['province', 'sex', 'status', 'age_cat','date_of_death', 'cause_of_death']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now drop any entries with unrecroded or icorrect dates of death and convert the time strings to python datetime objects.\n",
    "\n",
    "With all of those modifcations we can finally save this dataset as complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = scratch[scratch['province'].isin(picked)]\n",
    "scratch = scratch[scratch['date_of_death'] != '0000-00-00']\n",
    "scratch = scratch[scratch['date_of_death'] != '1970-01-01']\n",
    "scratch['date_of_death'] = pd.to_datetime(scratch['date_of_death'])\n",
    "\n",
    "save(scratch, 'clean_vdc_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly requires country codes in order to plot countries. We found a csv file of country codes that work with Plotly on [Kaggle](https://www.kaggle.com/shep312/plotlycountrycodes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv('plotly_countries_and_codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went through our refugee data and changed country names to make it compatible with the Plotly country names in order to join the two data frames. These are the countries we had to replace:\n",
    "- `Czech Rep. -> Czech Republic `\n",
    "- `United Kingdom of Great Britain and Northern Ireland -> `\n",
    "- `Rep. of Korea -> Korea, South  `\n",
    "- `The former Yugoslav Rep. of Macedonia -> Macedonia`\n",
    "- `USA (INS/DHS) -> United States   `\n",
    "- `USA (EOIR) -> United States`\n",
    "- `USA (EOIR) -> United States `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace names in refugee_subse dataframe\n",
    "new_refugee_month_df = refugee_month_df.replace({\n",
    "    'Czech Rep.': 'Czech Republic',\n",
    "    'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n",
    "    'Rep. of Korea': 'Korea, South',\n",
    "    'The former Yugoslav Rep. of Macedonia': 'Macedonia',\n",
    "    'USA (INS/DHS)': 'United States',\n",
    "    'USA (EOIR)': 'United States',\n",
    "    'Serbia and Kosovo: S/RES/1244 (1999)': 'Serbia'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge refugee data with Plotly country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining two tables together\n",
    "merged_month_df = new_refugee_month_df.set_index('Country / territory of asylum/residence').join(countries_df.set_index('COUNTRY'))\n",
    "merged_month_df = merged_month_df.reset_index()\n",
    "merged_month_df.rename(columns={'index': 'Country'}, inplace=True)\n",
    "\n",
    "# remove * and convert values to int values\n",
    "merged_month_df['Value'] = merged_month_df['Value'].apply(lambda x : 0 if x == '*' else int(x))\n",
    "\n",
    "# sum values and group each country by year\n",
    "total_month_df = pd.DataFrame(merged_month_df.groupby(['Country','Year','CODE']).agg({'Value':np.sum})).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add latitude and longitude for each country to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = pd.read_csv('countries.csv')\n",
    "\n",
    "total_month_sum_df= total_month_df.set_index('Country').join(locations_df.set_index('name'))\n",
    "total_month_sum_df = total_month_sum_df.reset_index()\n",
    "total_month_sum_df = total_month_sum_df.drop(['country'], axis=1)\n",
    "total_month_sum_df.rename(columns={'index':'Country'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Korea, South to South Korea\n",
    "name_change = {\n",
    "    'Korea, South': 'South Korea'\n",
    "}\n",
    "\n",
    "# renames the provinces using name_change\n",
    "total_month_sum_df.replace(name_change, inplace=True)\n",
    "\n",
    "# add coordinates for South Korea \n",
    "total_month_sum_df.at[[163,164,165,166,167,168,169,170], 'latitude'] = 35.9078\n",
    "total_month_sum_df.at[[163,164,165,166,167,168,169,170], 'longitude'] = 127.7669\n",
    "\n",
    "# add coordinates for and Macedonia\n",
    "total_month_sum_df.at[[202,203,204,205,206,207,208,209], 'latitude'] = 41.6086\n",
    "total_month_sum_df.at[[202,203,204,205,206,207,208,209], 'longitude'] = 21.7453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalize Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN values\n",
    "total_month_sum_df = total_month_sum_df.dropna(subset=['CODE'])\n",
    "\n",
    "# sort values in ascending order\n",
    "dropped_nan_month_df = total_month_sum_df[pd.notnull(total_month_sum_df['Country'])].sort_values('Year', ascending=True)\n",
    "dropped_nan_month_df = dropped_nan_month_df.reset_index()\n",
    "dropped_nan_month_df = dropped_nan_month_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went through our refugee data and changed country names to make it compatible with the Plotly country names in order to join the two data frames. These are the countries we had to adjust.\n",
    "- `Venezuela (Bolivarian Republic of) -> Venezuela           `\n",
    "- `Czech Rep. -> Czech Republic `\n",
    "- `Iran (Islamic Rep. of) -> Iran`\n",
    "- `The former Yugoslav Republic of Macedonia -> Macedonia`\n",
    "- `Bolivia (Plurinational State of) -> Bolivia              `\n",
    "- `Dominican Rep. -> Dominican Republic`\n",
    "- `Rep. of Korea -> Korea, South  `\n",
    "- `South Sudan -> Sudan`\n",
    "- `Dem. Rep. of the Congo -> Congo, Democratic Republic of the                `\n",
    "- `Central African Rep. -> Central African Republic`\n",
    "- `United States of America -> United States `\n",
    "- `Sint Maarten (Dutch part) -> Sint Maarten`\n",
    "- `Serbia and Kosovo (S/RES/1244 (1999)) -> Serbia `\n",
    "- `Rep. of Moldova -> Moldova`\n",
    "- `Syrian Arab Rep. -> Syria`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace names in refugee_subse dataframe\n",
    "new_refugee_df = refugee_subset.replace({\n",
    "    'Venezuela (Bolivarian Republic of)':'Venezuela',\n",
    "    'Czech Rep.':'Czech Republic',\n",
    "    'Iran (Islamic Rep. of)':'Iran',\n",
    "    'The former Yugoslav Republic of Macedonia':'Macedonia',\n",
    "    'Bolivia (Plurinational State of)':'Bolivia',\n",
    "    'Dominican Rep.':'Dominican Republic',\n",
    "    'Rep. of Korea': 'Korea, South',\n",
    "    'United Rep. of Tanzania':'Tanzania',\n",
    "    'China, Hong Kong SAR':'Hong Kong',\n",
    "    'South Sudan':'Sudan',\n",
    "    'Dem. Rep. of the Congo':'Congo, Democratic Republic of the',\n",
    "    'Central African Rep.':'Central African Republic',\n",
    "    'United States of America':'United States',\n",
    "    'Sint Maarten (Dutch part)':'Sint Maarten',\n",
    "    'Serbia and Kosovo (S/RES/1244 (1999))':'Serbia',\n",
    "    'Rep. of Moldova':'Moldova',\n",
    "    'Syrian Arab Rep.':'Syria', \n",
    "    'Russian Federation':'Russia',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge refugee data with Plotly country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the two tables together\n",
    "merged_df = new_refugee_df.set_index('Country / territory of asylum/residence').join(countries_df.set_index('COUNTRY'))\n",
    "\n",
    "# reset row index\n",
    "merged_df = merged_df.reset_index()\n",
    "\n",
    "# remove GDP column\n",
    "merged_df = merged_df.drop(['GDP (BILLIONS)'], axis=1)\n",
    "\n",
    "# rename index column to 'Country'\n",
    "merged_df.rename(columns={'index':'Country'}, inplace=True)\n",
    "\n",
    "# rearrange columns\n",
    "merged_df = merged_df[['CODE','Country','Year',\n",
    "                       'Refugees (incl. refugee-like situations)',\n",
    "                       'Asylum-seekers (pending cases)',\n",
    "                       'Total Population']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set contains NaN values and * for missing data. We need to change these to 0. We do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "\n",
    "filter_rows = ['Refugees (incl. refugee-like situations)','Asylum-seekers (pending cases)','Total Population']\n",
    "for i in filter_rows:\n",
    "    # Fill NaN values with 0\n",
    "    merged_df[i] = merged_df[i].fillna(0)\n",
    "    # remove * and convert values to int values\n",
    "    merged_df[i] = merged_df[i].apply(lambda x : 0 if x == '*' else int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provided is cummulative, so we select the last year available which is 2017 to represent data from 2011-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows where the refugee data is not 0\n",
    "new_total_df = merged_df[merged_df['Refugees (incl. refugee-like situations)'] != 0]\n",
    "\n",
    "year_df = new_total_df[new_total_df['Year'] == '2017']\n",
    "year_df = year_df.reset_index()\n",
    "year_df = year_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the latitude and longitude for each country so we are using from ?????."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load locations csv file into a dataframe\n",
    "locations_df = pd.read_csv('countries.csv')\n",
    "\n",
    "total_sum_df= year_df.set_index('Country').join(locations_df.set_index('name'))\n",
    "total_sum_df = total_sum_df.reset_index()\n",
    "total_sum_df = total_sum_df.drop(['country'], axis=1)\n",
    "total_sum_df.rename(columns={'index':'Country'}, inplace=True)\n",
    "\n",
    "total_sum_df.at[26,'CODE'] = 'CI'\n",
    "\n",
    "# # drop rows where CODE is none\n",
    "total_sum_df = total_sum_df.dropna(subset=['CODE','Refugees (incl. refugee-like situations)'])\n",
    "total_sum_df.to_csv('total_sum_refugee_data.csv',index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_nan_df = total_sum_df[pd.notnull(total_sum_df['Country'])]\n",
    "dropped_nan_df = dropped_nan_df.sort_values('Refugees (incl. refugee-like situations)', ascending=False)\n",
    "dropped_nan_df = dropped_nan_df.reset_index()\n",
    "dropped_nan_df = dropped_nan_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year-by-Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# call this so that running plot_bokeh won't create \n",
    "# a new window and results will be shown in notebook\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is provincial for VDC, we decided to use the provincial shapefile to map the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the shape file and save it as a geo data frame\n",
    "shp_file = os.path.join('syr_admin_shp_utf8_18219', 'syr_admin1.shp')\n",
    "map_df   = gpd.read_file(shp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the VDC csv file and save it as a pandas data frame\n",
    "dataset = pd.read_csv('vdc_data.csv', encoding='latin-1', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VDC data set's \"Date of death\" column contains the month, day, and year the person died, but we only wanted to see the yearly fluctuations. Therefore, we took a substring of the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data has month and day, so we took a substring of the year of death\n",
    "dataset['Year of Death'] = dataset['Date of death'].str[:4]\n",
    "\n",
    "# counts the number of times a province is in the dataset for a certain year\n",
    "province_count = dataset.groupby(['Province', 'Year of Death']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns to make data frame smaller\n",
    "simplified_df = province_count.drop(province_count.columns[1:], axis=1)\n",
    "simplified_df = simplified_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns to make data frame smaller\n",
    "simplified_df = province_count.drop(province_count.columns[1:], axis=1)\n",
    "simplified_df = simplified_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slider for Bokeh maps works with column titles, not row values, so we had to pivot the table. The once \"Date of death\" values became column titles and the count of Syrian casualties for that particular year and country became the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so years are columns rather than values\n",
    "year_as_column = simplified_df.pivot_table('Unnamed: 0', 'Province', 'Year of Death')\n",
    "year_as_column.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant years\n",
    "year_as_column = year_as_column.drop(['0000', '1970'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the province names in the VDC data set and the shapefile for Syria were not the same, so we had to go through the names manually and see which ones were different so that we could match the different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing province names by hand\n",
    "name_change = {\n",
    "    'Damascus Suburbs': 'Rural Damascus',\n",
    "    'Daraa': 'Dar\\'a',\n",
    "    'Deir Ezzor': 'Deir-ez-Zor',\n",
    "    'Hasakeh': 'Al-Hasakeh',\n",
    "    'Idlib': 'Idleb',\n",
    "    'Raqqa': 'Ar-Raqqa',\n",
    "    'Sweida': 'As-Sweida'\n",
    "}\n",
    "\n",
    "# renames the provinces using name_change\n",
    "year_as_column.replace(name_change, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining data from casualties (VDC) and geo data frame (shape file)\n",
    "merged = year_as_column.set_index('Province').join(map_df.set_index('NAME_EN'))\n",
    "merged.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant information\n",
    "# row where there were no data for geo data\n",
    "merged = merged.drop([10, 15], axis=0)\n",
    "# columns with information not pertaining to creating choropleth map\n",
    "merged.drop(merged.columns[9:16], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with plot_bokeh() is that it only takes immutable objects. However, the Pandas dataframe is mutable. Therefore, in order to bypass this problem, we decided to convert the Pandas dataframe into a GeoDataFrame. The following code describes that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe to GeoDataFrame\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = merged['geometry']\n",
    "merged_gdf = merged.drop(['geometry'], axis=1)\n",
    "crs = {'init': 'epsg:4326'}\n",
    "gdf = GeoDataFrame(merged_gdf, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# specify slider columns:\n",
    "slider_columns = [\"201%d\"%i for i in range(1, 8)]\n",
    "slider_range = range(2011, 2018)\n",
    "\n",
    "# make slider plot:\n",
    "gdf.plot_bokeh(\n",
    "    figsize=(900, 600),\n",
    "    slider=slider_columns,\n",
    "    slider_range=slider_range,\n",
    "    slider_name=\"Year\", \n",
    "    colormap='Inferno',\n",
    "    hovertool_columns=[\"Province\"],\n",
    "    title=\"Deaths in Syria\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year-by-Year with plotly and mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle and read the file\n",
    "import pickle\n",
    "final = pickle.load(open('./death_by_province_by_year.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://plot.ly/python/scattermapbox/\n",
    "# reference: https://community.periscopedata.com/t/36nz2s/plotly-choropleth-with-slider-map-charts-over-time\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# this is a public mapbox token\n",
    "mapbox_access_token = 'pk.eyJ1IjoibWF0dGhld2lydHoiLCJhIjoiY2p3ZTNpNXlnMHYxcjQ5bzdwMjc0anlpeSJ9.bSLA-SSqEomk0hC52rNliQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first year\n",
    "year = 2011\n",
    "\n",
    "# for every year we need the latitude, longitude, and the # of casualties\n",
    "# we use a for loop to store the data for each year of the Syrian War\n",
    "data_slider = []\n",
    "for year in final['year'].unique():\n",
    "    sect =  final[(final['year']== year)]\n",
    "\n",
    "    for col in sect.columns:\n",
    "        sect[col] = sect[col].astype(str)\n",
    "        \n",
    "    data_each_yr = go.Scattermapbox(\n",
    "        name=str(year),\n",
    "        lat=['36.2021',\n",
    "             '33.5138',\n",
    "             '33.5167',\n",
    "             '32.6264',\n",
    "             '35.3297',\n",
    "             '35.1409',\n",
    "             '36.5079',\n",
    "             '34.7324',\n",
    "             '35.9310',\n",
    "             '33.1219',\n",
    "             '35.5407',  \n",
    "             '35.9594',  \n",
    "             '32.7129', \n",
    "             '34.8959'],\n",
    "        lon=['37.1343',\n",
    "             '36.2765',\n",
    "             '36.9541',\n",
    "             '36.1033',\n",
    "             '40.1350',\n",
    "             '36.7552',\n",
    "             '40.7463',\n",
    "             '36.7137',\n",
    "             '36.6418',\n",
    "             '35.8209',\n",
    "             '35.7953', \n",
    "             '38.9981',  \n",
    "             '36.5663',\n",
    "             '35.8867'],\n",
    "        text=[\"Aleppo\",\n",
    "              \"Damascus\",\n",
    "              \"Rural Damascus\",\n",
    "              \"Daraa\",\n",
    "              \"Deir Ezzor\",\n",
    "              \"Hama\",\n",
    "              \"Hasakeh\",\n",
    "              \"Homs\",\n",
    "              \"Idlib\",\n",
    "              \"Lattakia\",\n",
    "              \"Quneitra\", \n",
    "              \"Ar Raqqah\", \n",
    "              \"As Suwayda\",\n",
    "              \"Tartus\"],\n",
    "        mode='markers',\n",
    "        marker = go.scattermapbox.Marker(\n",
    "            size = sect['casualties'].astype(int),\n",
    "            color = 'rgb(255, 0, 0)',\n",
    "            sizemode = 'area',\n",
    "        ), visible=False\n",
    "    )\n",
    "    data_slider.append(data_each_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the steps are for each of the possible places our slider can rest\n",
    "steps = []\n",
    "for i in range(len(data_slider)):\n",
    "    step = dict(method='restyle',\n",
    "                args=['visible', [False] * len(data_slider)],\n",
    "                label='Year {}'.format(i + 2011))\n",
    "    step['args'][1][i] = True\n",
    "    steps.append(step)\n",
    "    \n",
    "sliders = [dict(active=0, pad={\"t\": 1}, steps=steps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up our frame with the mapbox\n",
    "layout = go.Layout(\n",
    "    showlegend=False,\n",
    "    title = go.layout.Title(\n",
    "            text = '2011-2018 Syrian Casualties'\n",
    "        ),\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    sliders=sliders,\n",
    "    mapbox=go.layout.Mapbox(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=go.layout.mapbox.Center(\n",
    "            lat=34.7,\n",
    "            lon=37.2\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=5.4\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "fig = go.Figure(data=data_slider, layout=layout)\n",
    "py.iplot(fig, filename='Multiple-Mapbox.html')\n",
    "# plot(fig, filename='Multiple-Mapbox.html')\n",
    "# use ^ to have the graph open into a seperate tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day-by-Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>Day-by-Day</b> code is similar to the <b>Year-by-Year</b> code, so we hope the viewer doesn't mind if we have less comments in this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the shape file and save it as a geo data frame\n",
    "shp_file = os.path.join('syr_admin_shp_utf8_18219', 'syr_admin1.shp')\n",
    "map_df   = gpd.read_file(shp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle objects as a pandas data frame\n",
    "day_df = pickle.load(open('./death_by_province_by_day.pickle', 'rb'))\n",
    "\n",
    "# read the VDC csv file specifically for column \"geometry\"\n",
    "dataset = pd.read_csv('vdc_data.csv', encoding='latin-1', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing province names by hand\n",
    "name_change = {\n",
    "    'Damascus Suburbs': 'Rural Damascus',\n",
    "    'Daraa': 'Dar\\'a',\n",
    "    'Deir Ezzor': 'Deir-ez-Zor',\n",
    "    'Raqqa': 'Ar-Raqqa',\n",
    "    'Sweida': 'As-Sweida',\n",
    "    'Idlib': 'Idleb',\n",
    "    'Hasakeh': 'Al-Hasakeh',\n",
    "}\n",
    "\n",
    "# renames the provinces using name_change\n",
    "day_df.replace(name_change, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'day_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-e78142cc0abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make it so days are columns rather than values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpivoted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mday_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'casualties'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'province'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'day_df' is not defined"
     ]
    }
   ],
   "source": [
    "# make it so days are columns rather than values\n",
    "pivoted_df = day_df.pivot_table('casualties','province','day').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the data frames in order to obtain the geo data\n",
    "use = pivoted_df.join(map_df.set_index('NAME_EN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any unnecessary columns\n",
    "ready = use.drop(columns=['NAM_EN_REF','NAME_AR','PCODE','ADM0_EN','ADM0_AR','ADM0_PCODE','UPDATE_DAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of days from the first date of death to the last date of death is 2,686 days. Therefore, we created 2,686 columns for each day. (Note: this may take a while to finish running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"year-month-day time\" and replace it with the \"day\"\n",
    "for i in range(0, 2687):\n",
    "    ready = ready.rename(index=str, columns={ready.columns[i]: str(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframe to GeoDataFrame\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "geometry = ready['geometry']\n",
    "crs = {'init': 'epsg:4326'}\n",
    "day_gdf = GeoDataFrame(ready, crs=crs, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 'province' a column\n",
    "day_gdf = day_gdf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with our visualization is that most of the deaths are in the single to double digits. Very few times do the number of deaths on a single day go into the triple digits. However, the color bar is uniformly split, and we weren't able to find documentation on how to split up the bokeh color bar up, so we brute forced that colors in the color bar.<br><br>\n",
    "What we wanted to display was that between 0 and 100, the difference in hue would be greater than the difference in hue between 101-600. Therefore, there would be a bigger variety of colors on the map at a certain time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify slider columns\n",
    "slider_columns = []\n",
    "for i in range (0, 2687):\n",
    "    slider_columns.append(str(i))\n",
    "\n",
    "slider_range = range(0, 2687)\n",
    "\n",
    "# make slider plot\n",
    "day_gdf.plot_bokeh(\n",
    "    figsize=(900, 600),\n",
    "    slider=slider_columns,\n",
    "    slider_range=slider_range,\n",
    "    slider_name=\"Day\",\n",
    "    # brute force color bar for map\n",
    "    colormap=['#edf8f3', '#dcf2e8', '#cbebdd', '#b9e5d2', '#a8dfc7', '#97d8bc', '#85d2b1', '#74cba6', '#63c59b', '#52bf90',\n",
    "              '#52bf90', '#49ab81', '#419873', '#398564', '#317256', '#295f48', '#204c39', '#18392b', '#18392b', '#18392b', \n",
    "              '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', '#10261c', \n",
    "              '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812', '#0a1812',\n",
    "              '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', '#08140f', \n",
    "              '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', '#07110c', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', \n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "              '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e', '#08130e',\n",
    "    ],\n",
    "    hovertool_columns=[\"province\"],\n",
    "    title=\"Deaths in Syria\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day-by-Day Cumulative Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_bokeh\n",
    "import datetime\n",
    "pandas_bokeh.output_notebook()\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to read and write to pickle files\n",
    "def save(obj, name):\n",
    "    pickle.dump(obj, open(name + '.pickle', 'wb'))\n",
    "\n",
    "def load(name):\n",
    "    return pickle.load(open(name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up the map of syria\n",
    "shp_file = os.path.join('syr_admin_shp_utf8_18219', 'syr_admin1.shp')\n",
    "map_df   = gpd.read_file(shp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we change the names of some of the provinces\n",
    "name_change = {\n",
    "    'Damascus Suburbs': 'Rural Damascus',\n",
    "    'Daraa'           : 'Dar\\'a',\n",
    "    'Deir Ezzor'      : 'Deir-ez-Zor',\n",
    "    'Hasakeh'         : 'Al-Hasakeh',\n",
    "    'Idlib'           : 'Idleb',\n",
    "    'Raqqa'           : 'Ar-Raqqa',\n",
    "    'Sweida'          : 'As-Sweida'\n",
    "}\n",
    "\n",
    "dayFrame = load('death_by_province_by_day')\n",
    "dayFrame.replace(name_change, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we manipulate the dataframe to form a cumulative sum\n",
    "# fill in the NA values\n",
    "# put it in the form to be used with a slider in bokeh\n",
    "pivotedDF         = dayFrame.groupby(by=['province', 'day']).sum().groupby(level=[0]).cumsum()\n",
    "pivotedDF.columns = ['count']\n",
    "pivotedDF         = pivotedDF.pivot_table(index = 'day', columns = 'province', values = 'count')\n",
    "pivotedDF.iloc[0] = pivotedDF.iloc[0].fillna(0)\n",
    "pivotedDF         = pivotedDF.fillna(method='ffill')\n",
    "pivotedDF         = pivotedDF.transpose()\n",
    "pivotedDF         = pivotedDF.reset_index()\n",
    "pivotedDF.columns = [x.strftime('%m/%d/%Y') if type(x) is not str else x for x in pivotedDF.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with the GEO data\n",
    "use = pivotedDF.set_index('province').join(map_df.set_index('NAME_EN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change what the frame is indexed by\n",
    "use.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that aren't being used\n",
    "ready = use.drop(columns=['NAM_EN_REF','NAME_AR','PCODE','ADM0_EN','ADM0_AR','ADM0_PCODE','UPDATE_DAT'], axis=1)\n",
    "ready.index = list(ready.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our GeoDataFrame\n",
    "m_geometry   = ready['geometry']\n",
    "m_merged_gdf = ready.drop(['geometry'], axis=1)\n",
    "m_crs        = {'init': 'epsg:4326'}\n",
    "m_gdf        = GeoDataFrame(m_merged_gdf, crs=m_crs, geometry=m_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify slider columns:\n",
    "m_slider_columns = list(m_gdf.columns)[1:-1]\n",
    "m_slider_range   = range(0, 2687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make slider plot:\n",
    "m_gdf.plot_bokeh(\n",
    "    figsize           = (900, 600),\n",
    "    slider            = m_slider_columns,\n",
    "    slider_range      = m_slider_range,\n",
    "    slider_name       = \"Day\", \n",
    "    colormap          = \"Inferno\",\n",
    "    hovertool_columns = [\"province\"],\n",
    "    title             = \"Deaths in Syria\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-291-764a6336ab90>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-291-764a6336ab90>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Slider:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Slider:\n",
    "https://community.periscopedata.com/t/36nz2s/plotly-choropleth-with-slider-map-charts-over-time\n",
    "https://amaral.northwestern.edu/blog/step-step-how-plot-map-slider-represent-time-evolu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "ename": "PlotlyRequestError",
     "evalue": "Aw, snap! You tried to use our API as the user 'ksuhr1', but the supplied API key doesn't match our records. You can view your API key at https://plot.ly/settings/api\n\nYou're most likely getting this message because your local credentials file isn't synced with the Plotly server you're communicating with.\n\nGo to https://plot.ly/<language>/getting-started (e.g., https://plot.ly/python/getting-started) for more information.\n\nMake sure that you're logged in as ksuhr1.\n\nNeed help? Please try searching Plotly's <a href='http://stackoverflow.com/questions/tagged/plotly'>Stack Overflow channel</a>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPlotlyRequestError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-292-68976dfaa830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_slider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# fig = go.Figure(data = data_slider, layout = layout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'd3-world-map'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/plotly/plotly.py\u001b[0m in \u001b[0;36miplot\u001b[0;34m(figure_or_data, **plot_options)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'auto_open'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mplot_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auto_open'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_or_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplot_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_or_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/plotly/plotly.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(figure_or_data, validate, **plot_options)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mplot_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layout'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclientresp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplot_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# Check if the url needs a secret key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/api/v1/clientresp.py\u001b[0m in \u001b[0;36mclientresp\u001b[0;34m(data, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{plotly_domain}/clientresp'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Old functionality, just keeping it around.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mRetrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_reject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mdelay_since_first_attempt_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, wrap_exception)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/api/v1/utils.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No content'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPlotlyRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mvalidate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/api/v1/utils.py\u001b[0m in \u001b[0;36mvalidate_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No Content'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPlotlyRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPlotlyRequestError\u001b[0m: Aw, snap! You tried to use our API as the user 'ksuhr1', but the supplied API key doesn't match our records. You can view your API key at https://plot.ly/settings/api\n\nYou're most likely getting this message because your local credentials file isn't synced with the Plotly server you're communicating with.\n\nGo to https://plot.ly/<language>/getting-started (e.g., https://plot.ly/python/getting-started) for more information.\n\nMake sure that you're logged in as ksuhr1.\n\nNeed help? Please try searching Plotly's <a href='http://stackoverflow.com/questions/tagged/plotly'>Stack Overflow channel</a>."
     ]
    }
   ],
   "source": [
    "colorscale = [\n",
    "        [0,\"#E1F5FE\"], #50\n",
    "        [0.0001,\"#E1F5FE\"],\n",
    "        [0.0001,'#c4ebfc'],\n",
    "        [0.001,\"#c4ebfc\"], \n",
    "        [0.001,\"#B3E5FC\"], #100\n",
    "        [0.01,\"#B3E5FC\"],\n",
    "        [0.01,\"#81D4FA\"], #200\n",
    "        [0.1,\"#81D4FA\"],\n",
    "        [0.1,\"#4FC3F7\"], #300\n",
    "        [0.2,\"#4FC3F7\"],\n",
    "        [0.2,\"#29B6F6\"], #400\n",
    "        [0.3,\"#29B6F6\"],\n",
    "        [0.3,\"#03A9F4\"], #500\n",
    "        [0.4,\"#03A9F4\"],\n",
    "        [0.4,\"#039BE5\"], #600\n",
    "        [0.5,\"#039BE5\"],\n",
    "        [0.5,\"#0288D1\"], #700\n",
    "        [0.6,\"#0288D1\"],\n",
    "        [0.6,\"#0277BD\"], #800\n",
    "        [0.7,\"#0277BD\"], \n",
    "        [0.7,\"#01579B\"], #900\n",
    "        [0.8,\"#01579B\"], \n",
    "        [0.8,\"#00457c\"], #1000\n",
    "        [0.9,\"#00457c\"],\n",
    "        [0.9,\"#002f56\"], #1100\n",
    "        [1.0,\"#002f56\"],  \n",
    "    ]\n",
    "\n",
    "data_slider = []\n",
    "for year in dropped_nan_month_df['Year'].unique():\n",
    "    # break apart data for each year\n",
    "    df_segmented =  dropped_nan_month_df[(dropped_nan_month_df['Year']== year)]\n",
    "    for col in df_segmented.columns:\n",
    "        df_segmented[col] = df_segmented[col].astype(str)\n",
    "    \n",
    "    data_each_yr= dict(\n",
    "                        type='choropleth',\n",
    "                        locations = df_segmented['CODE'],\n",
    "                        z=df_segmented['Value'],\n",
    "                        zmin = 0,\n",
    "                        zmax = 300000,\n",
    "                        locationmode='europe',\n",
    "                        autocolorscale=False,\n",
    "                        text =df_segmented['Country'],\n",
    "                        colorscale = colorscale,\n",
    "                        colorbar= {'title':'Number of Syrian Refugees'})\n",
    "    \n",
    "    #flight paths section\n",
    "    text_data = df_segmented['Country']+\": \"+df_segmented['Value']\n",
    "    countries = [ dict(\n",
    "        type = 'scattergeo',\n",
    "#         locationmode = 'europe',\n",
    "        lon = df_segmented['longitude'],\n",
    "        lat = df_segmented['latitude'],\n",
    "        hoverinfo = 'text',\n",
    "        text = text_data,\n",
    "        mode = 'markers',\n",
    "         marker = dict(\n",
    "            size=2,\n",
    "            color='rgb(255, 0, 0)',\n",
    "            line = dict(\n",
    "                width=3,\n",
    "                color='rgba(68, 68, 68, 0)'\n",
    "            ))\n",
    "    )]\n",
    "    \n",
    "    \n",
    "    refugee_paths = []\n",
    "    syria_start_lon = 38.996815\n",
    "    syria_start_lat = 34.802075\n",
    "    maximum_value = float(df_segmented['Value'].max())\n",
    "    for i in range( len( df_segmented ) ):\n",
    "#         print(\"i:\",i)\n",
    "        opacity = 0\n",
    "        \n",
    "        country_refugee = df_segmented['Value'].iloc[i]\n",
    "        opacity = float(country_refugee) / maximum_value\n",
    "        #doing this because otherwise, the lines to countries besides Germany are invisible\n",
    "        if opacity < 0.25:\n",
    "            opacity = 0.25\n",
    "\n",
    "        refugee_paths.append(\n",
    "            dict(\n",
    "                type = 'scattergeo',\n",
    "#                 locationmode = 'USA-states',\n",
    "                lon = [ syria_start_lon, df_segmented['longitude'].iloc[i] ],\n",
    "                lat = [ syria_start_lat, df_segmented['latitude'].iloc[i] ],\n",
    "                mode = 'lines',\n",
    "                line = dict(\n",
    "                    width = 1,\n",
    "                    color = 'red',\n",
    "                ),\n",
    "                opacity=opacity,\n",
    "\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    data_slider.append(data_each_yr)\n",
    "# https://amaral.northwestern.edu/blog/step-step-how-plot-map-slider-represent-time-evolu\n",
    "# create steps for the slider\n",
    "steps = []\n",
    "for i in range(len(data_slider)):\n",
    "    step = dict(method='restyle',\n",
    "                args=['visible', [False] * len(data_slider)],\n",
    "                label='{}'.format(i + 2011))\n",
    "    step['args'][1][i] = True\n",
    "    steps.append(step)\n",
    "    \n",
    "# create 'sliders' object from the 'steps'\n",
    "sliders = [dict(active=0, pad={\"t\": 1}, steps=steps)]\n",
    "\n",
    "layout = dict(title ='2011-2018 Syrian Refugee Migration Patterns',\n",
    "              geo=dict(\n",
    "                  projection=dict(type='equirectangular'),\n",
    "                  showland=True,\n",
    "                  landcolor='rgb(243,243,243)',\n",
    "                  countrycolor='rgb(204,204,204)',\n",
    "                  scope='europe'\n",
    "              ),\n",
    "              sliders=sliders)\n",
    "\n",
    "fig = dict(data=data_slider, layout=layout)\n",
    "# fig = go.Figure(data = data_slider, layout = layout)\n",
    "py.iplot(fig, validate = False, filename = 'd3-world-map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See how flow of Refugees change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment out print statements to see top \n",
    "# countries of most migrations for each year 2011-2018\n",
    "for year in dropped_nan_month_df['Year'].unique():\n",
    "    year_df =  dropped_nan_month_df[(dropped_nan_month_df['Year']== year)]\n",
    "    country_df = year_df.sort_values('Value', ascending=False)\n",
    "    country_df = country_df.reset_index()\n",
    "#     country_df = country_df.drop(['index'], axis=1)\n",
    "#     print(country_df[:5])\n",
    "#     print(\"\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Choropleth:\n",
    "https://plot.ly/python/choropleth-maps/\n",
    "\n",
    "Flight paths:\n",
    "https://plot.ly/pandas/lines-on-maps/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "PlotlyRequestError",
     "evalue": "Aw, snap! You tried to use our API as the user 'ksuhr1', but the supplied API key doesn't match our records. You can view your API key at https://plot.ly/settings/api\n\nYou're most likely getting this message because your local credentials file isn't synced with the Plotly server you're communicating with.\n\nGo to https://plot.ly/<language>/getting-started (e.g., https://plot.ly/python/getting-started) for more information.\n\nMake sure that you're logged in as ksuhr1.\n\nNeed help? Please try searching Plotly's <a href='http://stackoverflow.com/questions/tagged/plotly'>Stack Overflow channel</a>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPlotlyRequestError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-398a1de80ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mrefugee_paths\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'd3-world-map'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/plotly/plotly.py\u001b[0m in \u001b[0;36miplot\u001b[0;34m(figure_or_data, **plot_options)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'auto_open'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mplot_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auto_open'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_or_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplot_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_or_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/plotly/plotly.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(figure_or_data, validate, **plot_options)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mplot_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layout'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclientresp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplot_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# Check if the url needs a secret key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/api/v1/clientresp.py\u001b[0m in \u001b[0;36mclientresp\u001b[0;34m(data, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{plotly_domain}/clientresp'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Old functionality, just keeping it around.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mRetrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_reject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mdelay_since_first_attempt_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, wrap_exception)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/retrying.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattempt_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/api/v1/utils.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No content'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPlotlyRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mvalidate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/chart_studio/api/v1/utils.py\u001b[0m in \u001b[0;36mvalidate_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No Content'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPlotlyRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPlotlyRequestError\u001b[0m: Aw, snap! You tried to use our API as the user 'ksuhr1', but the supplied API key doesn't match our records. You can view your API key at https://plot.ly/settings/api\n\nYou're most likely getting this message because your local credentials file isn't synced with the Plotly server you're communicating with.\n\nGo to https://plot.ly/<language>/getting-started (e.g., https://plot.ly/python/getting-started) for more information.\n\nMake sure that you're logged in as ksuhr1.\n\nNeed help? Please try searching Plotly's <a href='http://stackoverflow.com/questions/tagged/plotly'>Stack Overflow channel</a>."
     ]
    }
   ],
   "source": [
    "data = [go.Choropleth(\n",
    "    locations = dropped_nan_df['CODE'],\n",
    "    z = dropped_nan_df['Refugees (incl. refugee-like situations)'],\n",
    "    text = dropped_nan_df['Country'],\n",
    "    colorscale = [\n",
    "        [0,\"#E1F5FE\"], #50\n",
    "        [0.0001,\"#E1F5FE\"],\n",
    "        [0.0001,'#c4ebfc'],\n",
    "        [0.001,\"#c4ebfc\"], \n",
    "        [0.001,\"#B3E5FC\"], #100\n",
    "        [0.01,\"#B3E5FC\"],\n",
    "        [0.01,\"#81D4FA\"], #200\n",
    "        [0.1,\"#81D4FA\"],\n",
    "        [0.1,\"#4FC3F7\"], #300\n",
    "        [0.2,\"#4FC3F7\"],\n",
    "        [0.2,\"#29B6F6\"], #400\n",
    "        [0.3,\"#29B6F6\"],\n",
    "        [0.3,\"#03A9F4\"], #500\n",
    "        [0.4,\"#03A9F4\"],\n",
    "        [0.4,\"#039BE5\"], #600\n",
    "        [0.5,\"#039BE5\"],\n",
    "        [0.5,\"#0288D1\"], #700\n",
    "        [0.6,\"#0288D1\"],\n",
    "        [0.6,\"#0277BD\"], #800\n",
    "        [0.7,\"#0277BD\"], \n",
    "        [0.7,\"#01579B\"], #900\n",
    "        [0.8,\"#01579B\"], \n",
    "        [0.8,\"#00457c\"], #1000\n",
    "        [0.9,\"#00457c\"],\n",
    "        [0.9,\"#002f56\"], #1100\n",
    "        [1.0,\"#002f56\"],  \n",
    "    ],\n",
    "       \n",
    "    zmin = 0,\n",
    "    #zmax = 600000,\n",
    "    zmax = 1000000,\n",
    "    autocolorscale = False,\n",
    "    reversescale = False,\n",
    "    marker = go.choropleth.Marker(\n",
    "        line = go.choropleth.marker.Line(\n",
    "            color = 'rgb(180,180,180)',\n",
    "            width = 0.5\n",
    "        )),\n",
    "    colorbar = go.choropleth.ColorBar(\n",
    "        tickprefix = '',\n",
    "        title = 'Number of Syrian Refugees'),\n",
    "\n",
    "\n",
    ")]\n",
    "\n",
    "# following code is for lines connecting countries\n",
    "countries = [go.Scattergeo(\n",
    "    lon = total_sum_df['longitude'],\n",
    "    lat = total_sum_df['latitude'],\n",
    "    hoverinfo = 'text',\n",
    "    text = total_sum_df['Country'],\n",
    "    mode = 'none',\n",
    "#     marker = go.scattergeo.Marker(\n",
    "#         size = 0,\n",
    "#         color = 'rgb(255, 0, 0)',\n",
    "#         line = go.scattergeo.marker.Line(\n",
    "#             width = 3,\n",
    "#             color = 'rgba(68, 68, 68, 0)'\n",
    "#         ))\n",
    "    )]\n",
    "\n",
    "refugee_paths = []\n",
    "\n",
    "syria_start_lon = 38.996815\n",
    "syria_start_lat = 34.802075\n",
    "maximum_value = float(dropped_nan_df['Refugees (incl. refugee-like situations)'].max())\n",
    "\n",
    "for i in range(len(dropped_nan_df)):\n",
    "    # setting the opacity of the lines\n",
    "    opacity = 0\n",
    "    country_refugee = dropped_nan_df['Refugees (incl. refugee-like situations)'][i]\n",
    "    opacity = float(country_refugee) / maximum_value\n",
    "    # doing this because otherwise, the lines to countries besides Germany are invisible\n",
    "    if opacity < 0.25:\n",
    "        opacity = 0.25\n",
    "        \n",
    "    refugee_paths.append(\n",
    "        go.Scattergeo(\n",
    "            locationmode = 'country names',\n",
    "            lon = [syria_start_lon, dropped_nan_df['longitude'][i]],\n",
    "            lat = [syria_start_lat, dropped_nan_df['latitude'][i]],\n",
    "            mode = 'lines',\n",
    "            line = go.scattergeo.Line(\n",
    "                width = 1,\n",
    "                color = 'red',\n",
    "            ),\n",
    "            opacity = opacity,\n",
    "        )\n",
    "    )\n",
    "\n",
    "layout = go.Layout(\n",
    "    showlegend = False,\n",
    "    title = go.layout.Title(\n",
    "        text = '2011-2018 Syrian Refugee Migration Patterns'\n",
    "    ),\n",
    "    \n",
    "    geo = go.layout.Geo(\n",
    "        showframe = False,\n",
    "        showcoastlines = False,\n",
    "        projection = go.layout.geo.Projection(\n",
    "            type = 'equirectangular',\n",
    "        ),\n",
    "        showland = True,\n",
    "        landcolor = 'rgb(243, 243, 243)',\n",
    "        countrycolor = 'rgb(204, 204, 204)', \n",
    "    ),\n",
    "    \n",
    "    annotations = [go.layout.Annotation(\n",
    "        x = 0.55,\n",
    "        y = 0.1,\n",
    "        xref = 'paper',\n",
    "        yref = 'paper',\n",
    "        text = 'Source: <a href=\"http://popstats.unhcr.org/en/asylum_seekers_monthly\">\\\n",
    "            Asylum-Seekers (Monthly Data)</a>',\n",
    "        showarrow = False\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data +refugee_paths , layout = layout)\n",
    "py.iplot(fig, validate = False, filename = 'd3-world-map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_countries = dropped_nan_df.sort_values('Refugees (incl. refugee-like situations)', ascending=False)\n",
    "max_countries.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/barh.html\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# y_data will be the titles\n",
    "y_data = []\n",
    "\n",
    "max_countries = max_countries[:10]\n",
    "x_data = max_countries['Refugees (incl. refugee-like situations)']\n",
    "y_data = max_countries['Country']\n",
    "\n",
    "# plot data\n",
    "ax.barh(y_data, x_data, color='k', align='center')\n",
    "\n",
    "# label data\n",
    "ax.set_yticks(y_data)\n",
    "ax.set_yticklabels(y_data)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Number of Refugees')\n",
    "ax.set_title('Top Countries With Greatest Amount of Refugees')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
