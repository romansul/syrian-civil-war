{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import schedule\n",
    "import smtplib\n",
    "import pickle\n",
    "import time\n",
    "import tqdm\n",
    "import glob\n",
    "import ssl\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "from collections    import defaultdict\n",
    "from statistics     import mean\n",
    "from torrequest     import TorRequest\n",
    "from random         import randint\n",
    "from bs4            import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few goals with this notebook:\n",
    "1. Scrape a website using BeautifulSoup\n",
    "2. Use Tor to avoid IP detection\n",
    "3. Randomly switch user agents to avoid detection\n",
    "4. Schedule scraping to happen automatically every day\n",
    "5. Email ourselves with information about scraping\n",
    "\n",
    "The website that we'll be scraping can be found [here](http://www.vdc-sy.info/index.php/en/martyrs). They provide their information with in a dataframe-like structure, but unfortunately it doesn't display all the information they have per entry. If we click on a name we are brought to another page displaying more detailed information on the entry. We are lucky that the data we want to scrape from this site can be accessed directly by modifying the end of the url with a number up to around 250,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, we'll need to install Tor and configure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing Tor, we'll want to change our password associated with the Port that Tor uses. To do this, we just enter tor --hash-password\n",
    "\n",
    "Make sure to copy the hashed password!\n",
    "\n",
    "Now we need to make some changes to our Tor configuration file. In Mac OS, it can be found in usr/etc/tor/torrc\n",
    "\n",
    "We have to uncomment lines 57, 60, and 61.\n",
    "\n",
    "In the HashedControlPassword field, paste the hashed password you just got earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torrequests is the library that we'll use in order to send requests over the Tor Network. It is a very simple wrapper for the regular requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torrequest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can install fake-useragent, which will let us cycle through new useragents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll write a couple helper functions that make it easier to save files locally. This will let us resume progress if any errors kill our script. These files will be saved to the local directory as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(obj, name):\n",
    "    pickle.dump(obj, open(name + '.pickle', 'wb'))\n",
    "\n",
    "def load(name):\n",
    "    return pickle.load(open(name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the scraping. This will be relatively easy. We will get the page using a TorRequests to mask our IP, navigate to the data we need using BeautifulSoup, create a dataframe from that data, and save it to a file.\n",
    "\n",
    "The function scrape_recent() will handle the scraping for the most recent ids. It does this by finding all links that match a regex pattern and then checking these against a list of all previous ids. We'll use the next function to handle the rest.\n",
    "\n",
    "The function scrape_details() will handle scraping the pages on the website that contain the detailed information of an entry. This represents **one** person. Each person is saved and returned as a dataframe, since a person will have varying amounts and types of labels/columns and will have to be combined into one large dataset later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent():\n",
    "    first_page = 'http://www.vdc-sy.info/index.php/en/martyrs/1/c29ydGJ5PWEua2lsbGVkX2RhdGV8c29ydGRpcj1ERVNDfGFwcHJvdmVkPXZpc2libGV8ZXh0cmFkaXNwbGF5PTB8'\n",
    "    pattern    = re.compile('\\/index\\.php\\/en\\/details\\/martyrs\\/.')\n",
    "\n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    tor        = TorRequest(password = 'commonhorse')\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = tor.get(first_page, headers=headers)\n",
    "        content  = bs(response.text, 'html.parser')\n",
    "        \n",
    "        links    = {link['href'][30:] for link in content.find_all('a', href = True) if pattern.match(link['href'])} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_details(uid, tor, headers):\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    url  = 'http://www.vdc-sy.info/index.php/en/details/martyrs/' + uid\n",
    "    \n",
    "    # Headers will provide the UserAgent to use when getting response\n",
    "    # Makes the request using a TorRequest object passed in\n",
    "    page = tor.get(url, headers = headers).text\n",
    "    page = bs(page, 'html.parser')\n",
    "    \n",
    "    table = page.find('table', attrs = {'class':'peopleListing'})\n",
    "    rows  = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "\n",
    "        # All data without only 2 data values\n",
    "        # are not data we are looking for\n",
    "        if len(data) != 2:\n",
    "            continue\n",
    "\n",
    "        # data[0] corresponds to the row label/column\n",
    "        cols.append(data[0].text)\n",
    "        \n",
    "        # Values need to appended differently for image rows \n",
    "        if data[1].find('img') is not None:\n",
    "            vals.append(data[1].find('img')['src'])\n",
    "        else:\n",
    "            vals.append(data[1].text)\n",
    "\n",
    "    # Adds the uid to the dataframe\n",
    "    cols.append('uid')\n",
    "    vals.append(uid)\n",
    "\n",
    "    # Creates and saves dataframe\n",
    "    person = pd.DataFrame([vals], columns = cols, dtype=str)\n",
    "#     print(person.head())\n",
    "    save(person, uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can prepare the function to send an email. We'll have to change our Google Account settings first. We'll then have to turn on the 'Less secure app access' in the Security tab. This will allow us to log in and send emails through an account using python.\n",
    "\n",
    "Both the sll and smtplib librariers are part of standard Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from https://realpython.com/python-send-email/\n",
    "\"\"\"\n",
    "import ssl\n",
    "import smtplib\n",
    "\n",
    "def send_email():\n",
    "    # We must use this port for ssl\n",
    "    port     = 465\n",
    "    password = get_password()\n",
    "\n",
    "    sender_email   = \"romanlosul@gmail.com\"  \n",
    "    receiver_email = \"rsul@ucsc.edu\"  \n",
    "\n",
    "    message = \"\"\"\\\n",
    "    Subject: VDC Scrape Log\n",
    "\n",
    "    Scraping was a success.\n",
    "    \"\"\"\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init_uids()\n",
    "# uids_to_scrape = load('uids_to_scrape')\n",
    "# uids_scraped   = load('uids_scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tor without changing the our user agent is useless, since we can still be identified by our User Agent.\n",
    "\n",
    "We will use the [fake-useragent](https://github.com/hellysmile/fake-useragent) library to cycle through random user agents. There are issues with other approaches using User Agents that are out of date or uncommon, but this library User Agent selection is based on usage statistics from [http://useragentstring.com/](http://useragentstring.com/).\n",
    "\n",
    "Our process flow can be described as:\n",
    "    1. Get a random UserAgent\n",
    "    2. Create TorRequest instance\n",
    "    3. Remove uid from Queue\n",
    "    4. Scrape details with uid\n",
    "    5. Save progress\n",
    "    5. Repeat 3-5 until Queue is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    \n",
    "    # This is slightly modified from the UserAgent docs\n",
    "    # It creates a UserAgent Object, and assigns\n",
    "    # a random UserAgent to a header dict\n",
    "    \n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    \n",
    "    # Setting up Torrequest\n",
    "    # Followed instructions from \n",
    "    # https://www.scrapehero.com/make-anonymous-requests-using-tor-python/\n",
    "    \n",
    "    # This password is what we used earlier when setting up Tor\n",
    "    # TorRequest gives us a new IP address\n",
    "    tor = TorRequest(password = 'commonhorse')\n",
    "    \n",
    "    # reset_identity() should reset our IP Address, but \n",
    "    # it currently has a bug. We will keep it here for when \n",
    "    # TorRequests is updated. In the mean time, instantiate \n",
    "    # a new TorRequest object to get a new IP address.\n",
    "    tor.reset_identity()\n",
    "    \n",
    "    # We will load in the uids that we've already scraped\n",
    "    # and scrape the first page to get recent ids\n",
    "    scraped_uids   = load('scraped_uids')\n",
    "    recent_uids    = scrape_recent()\n",
    "    uids_to_scrape = []\n",
    "    \n",
    "    # We'll add any ids that we haven't already seen\n",
    "    # And then scrape the detailed pages\n",
    "    for uid in recent_uids:\n",
    "        if uid not in scraped_uids:\n",
    "            uids_to_scrape.append(uid)\n",
    "    \n",
    "    # We pop a uid off our list of uids to scrape and\n",
    "    # attempt to scrape it the detailed page. If it fails, \n",
    "    # then it adds it back to the queue and resets the UserAgent\n",
    "    # and Tor identity.\n",
    "    #\n",
    "    # If it succeeds then it adds the uid to the list of scraped uids \n",
    "    # and saves that file to load in later\n",
    "    \n",
    "    while len(uids_to_scrape) > 0:\n",
    "        uid = uids_to_scrape.pop()\n",
    "\n",
    "        try:\n",
    "            scrape_details(uid, tor, headers)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            uids_to_scrape.add(uid)\n",
    "            \n",
    "            ua         = UserAgent()\n",
    "            headers    = {'User-Agent': ua.random}\n",
    "            tor = TorRequest(password = 'commonhorse')\n",
    "            tor.reset_identity()\n",
    "        \n",
    "            continue\n",
    "\n",
    "        print('Left to scrape: ', len(uids_to_scrape))\n",
    "        scraped_uids.add(uid)\n",
    "        save(scraped_uids, 'scraped_uids')\n",
    "        \n",
    "    send_email()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now schedule our scraping to happen at 12 AM everyday, so long as this notebook is up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-22ca3c4b70f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \"\"\"\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mdefault_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m     92\u001b[0m         \u001b[0mrunnable_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mbetween\u001b[0m \u001b[0mbut\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mrunnable_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/schedule/__init__.py\u001b[0m in \u001b[0;36mshould_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "schedule.every().day.at(\"17:53\").do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_password():\n",
    "    return 'dipping1_mitzvoth'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
