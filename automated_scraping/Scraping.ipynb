{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import schedule\n",
    "import smtplib\n",
    "import pickle\n",
    "import time\n",
    "import tqdm\n",
    "import glob\n",
    "import ssl\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "from collections    import defaultdict\n",
    "from statistics     import mean\n",
    "from torrequest     import TorRequest\n",
    "from random         import randint\n",
    "from bs4            import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(obj, name):\n",
    "    pickle.dump(obj, open(name + '.pickle', 'wb'))\n",
    "\n",
    "def load(name):\n",
    "    return pickle.load(open(name + '.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent():\n",
    "    first_page = 'http://www.vdc-sy.info/index.php/en/martyrs/1/c29ydGJ5PWEua2lsbGVkX2RhdGV8c29ydGRpcj1ERVNDfGFwcHJvdmVkPXZpc2libGV8ZXh0cmFkaXNwbGF5PTB8'\n",
    "    pattern    = re.compile('\\/index\\.php\\/en\\/details\\/martyrs\\/.')\n",
    "\n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    tor        = TorRequest(password = 'commonhorse')\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = tor.get(first_page, headers=headers)\n",
    "        content  = bs(response.text, 'html.parser')\n",
    "        \n",
    "        links    = {link['href'][30:] for link in content.find_all('a', href = True) if pattern.match(link['href'])} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_details(uid, tor, headers):\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    url  = 'http://www.vdc-sy.info/index.php/en/details/martyrs/' + uid\n",
    "    \n",
    "    # Headers will provide the UserAgent to use when getting response\n",
    "    # Makes the request using a TorRequest object passed in\n",
    "    page = tor.get(url, headers = headers).text\n",
    "    page = bs(page, 'html.parser')\n",
    "    \n",
    "    table = page.find('table', attrs = {'class':'peopleListing'})\n",
    "    rows  = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "\n",
    "        # All data without only 2 data values\n",
    "        # are not data we are looking for\n",
    "        if len(data) != 2:\n",
    "            continue\n",
    "\n",
    "        # data[0] corresponds to the row label/column\n",
    "        cols.append(data[0].text)\n",
    "        \n",
    "        # Values need to appended differently for image rows \n",
    "        if data[1].find('img') is not None:\n",
    "            vals.append(data[1].find('img')['src'])\n",
    "        else:\n",
    "            vals.append(data[1].text)\n",
    "\n",
    "    # Adds the uid to the dataframe\n",
    "    cols.append('uid')\n",
    "    vals.append(uid)\n",
    "\n",
    "    # Creates and saves dataframe\n",
    "    person = pd.DataFrame([vals], columns = cols, dtype=str)\n",
    "#     print(person.head())\n",
    "    save(person, os.path.join('leftovers', uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helen___uids_to_scrape = load('helen___uids_to_scrape')\n",
    "katelyn_uids_to_scrape = load('katelyn_uids_to_scrape')\n",
    "matthew_uids_to_scrape = load('matthew_uids_to_scrape')\n",
    "roman___uids_to_scrape = load('roman___uids_to_scrape')\n",
    "\n",
    "helen___uids_scraped   = load('helen___uids_scraped')\n",
    "katelyn_uids_scraped   = load('katelyn_uids_scraped')\n",
    "matthew_uids_scraped   = load('matthew_uids_scraped')\n",
    "roman___uids_scraped   = load('roman___uids_scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua         = UserAgent()\n",
    "headers    = {'User-Agent': ua.random}\n",
    "tor        = TorRequest(password = 'commonhorse')\n",
    "tor.reset_identity()\n",
    "\n",
    "helen___uids_scraped   = load('helen___uids_scraped')\n",
    "helen___uids_to_scrape = load('helen___uids_to_scrape')\n",
    "\n",
    "while len(helen___uids_to_scrape) > 0:\n",
    "    uid = helen___uids_to_scrape.pop()\n",
    "\n",
    "    try:\n",
    "        scrape_details(uid, tor, headers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        helen___uids_to_scrape.append(uid)\n",
    "\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'commonhorse')\n",
    "        tor.reset_identity()\n",
    "\n",
    "        continue\n",
    "\n",
    "    print('Left to scrape: ', len(helen___uids_to_scrape))\n",
    "    helen___uids_scraped.add(uid)\n",
    "\n",
    "    save(helen___uids_to_scrape, 'helen___uids_to_scrape')\n",
    "    save(helen___uids_scraped  , 'helen___uids_scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matthew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua         = UserAgent()\n",
    "headers    = {'User-Agent': ua.random}\n",
    "tor        = TorRequest(password = '')\n",
    "tor.reset_identity()\n",
    "\n",
    "matthew_uids_scraped   = load('matthew_uids_scraped')\n",
    "matthew_uids_to_scrape = load('matthew_uids_to_scrape')\n",
    "\n",
    "while len(matthew_uids_to_scrape) > 0:\n",
    "    uid = matthew_uids_to_scrape.pop()\n",
    "\n",
    "    try:\n",
    "        scrape_details(uid, tor, headers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        matthew_uids_to_scrape.append(uid)\n",
    "\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = '')\n",
    "        tor.reset_identity()\n",
    "\n",
    "        continue\n",
    "\n",
    "    print('Left to scrape: ', len(matthew_uids_to_scrape))\n",
    "    matthew_uids_scraped.add(uid)\n",
    "\n",
    "    save(matthew_uids_to_scrape, 'matthew_uids_to_scrape')\n",
    "    save(matthew_uids_scraped  , 'matthew_uids_scraped')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
