{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casualties and Migration in the Syrian Civil War"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "***\n",
    "\n",
    "In 2011, five weeks into the civil demonstrations against the Syrian government, secret police forces detained and tortured fifteen students who had spray painted an anti-government statement on the walls of their school. They would be released weeks later in an effort to quell the rising civil unrest in the province. In the wake of the hundreds of other demonstrators who were killed or disappeared, this action was too little and too late to stop the tide of the civil war. Demonstrations turned to protest turned to armed conflict and the rest is history.\n",
    "\n",
    "The war would go on to spawn both the largest refugee crisis and one of the deadliest conflicts in modern history. As of 2019, there are over 6 million Syrian refugees and another 6 million internally displaced people in a country with a pre-war population of around 24 million (UNHCR, 2018). The regime's efforts to prevent accurate information from leaving the country has made it nearly impossible to estimate the number of casualties that have occured in that time. Current estimates range from 300,000 to 600,000 killed depending on the source.\n",
    "\n",
    "The link between the flow of violence within the country and the flow of asylum seekers out of the country should be apparent to anyone who is aware of the war. Yet a growing sentiment among residents in host countries is that a large portion of asylum seekers from Syria are actually economic migrants, who are using the conflict as a means of gaining entry into the European Union and access to generous social programs.\n",
    "\n",
    "We believe that violence is the most important predictor of migration of Syrian refugees; however, while this argument may be generally accepted, there is great difficulty in proving this relationship for certain. We hope to answer this question using reported casualty data to see whether there is a correlation between violence in a given province and a subsequent increase in the amount of asylum seekers across all host countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project\n",
    "***\n",
    "\n",
    "Our project can be organized into three distinct portions:\n",
    "\n",
    "1. Data Scraping\n",
    "2. Data Wrangling\n",
    "3. Data Visualization\n",
    "\n",
    "Our goal is to create a dataset for casualty information a refugee data, clean and structure the dataset for easier queying, and visualize the data to provide more insights into the questions we pose above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping\n",
    "***\n",
    "\n",
    "There are multiple sources that could be used for casualty information (list here). We will leave the three datasets for now, and focus on the VDC and CSR datasets because they provide their data is table elements that make it easy for us to scrape and organize our dataframes for analysis.\n",
    "\n",
    "We will now go through the process of scraping and creating the inital forms of these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Violations Documentation Center](http://www.vdc-sy.info/) has been recording casualty data since June 2011. It is likely the most detailed and complete (in terms of metadata) data source of casualties that is publicly accessible.\n",
    "\n",
    "They provide their data with a user interface that will query their database using parameter the user defines. This interface will provide this information:\n",
    "\n",
    "- `Name                  - Full name in English`\n",
    "- `Status                - Civilian, non-civilian, or military status of deceased`\n",
    "- `Sex                   - Whether deceased is an Adult or Minor and Male or Female`\n",
    "- `Province              - One of the 14 Provinces of Syria`\n",
    "- `Area \\ Place of Birth - Various locations that can be Provinces/Subdistricts/Towns`\n",
    "- `Date of death         - self explanatory`\n",
    "- `Cause of death        - self explanatory`\n",
    "- `Actors                - groups involved in the casualty`\n",
    "\n",
    "Each entry is associated with a unique identifier, which is an integer between 0 and 250,000. Clicking on the name of the entry will lead the user to another page that provides the unique identifier number and other data that is not displayed on the main page. We will avoid describing this detail for now, since most of this data is not used in the final product.\n",
    "\n",
    "We will describe the full process we used to scrape all details from this website as well as the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recent():\n",
    "    first_page = 'http://www.vdc-sy.info/index.php/en/martyrs/1/c29ydGJ5PWEua2lsbGVkX2RhdGV8c29ydGRpcj1ERVNDfGFwcHJvdmVkPXZpc2libGV8ZXh0cmFkaXNwbGF5PTB8'\n",
    "    \n",
    "    # This is the format of the links that give us the unique identfiers\n",
    "    pattern    = re.compile('\\/index\\.php\\/en\\/details\\/martyrs\\/.')\n",
    "\n",
    "    # We want to establish a randomized user agent and Tor node to avoid detection\n",
    "    ua         = UserAgent()\n",
    "    headers    = {'User-Agent': ua.random}\n",
    "    tor        = TorRequest(password = 'commonhorse')\n",
    "    \n",
    "    try:\n",
    "        response = tor.get(first_page, headers=headers)\n",
    "        content  = bs(response.text, 'html.parser')\n",
    "        \n",
    "        # This list comprehension grabs all unique identifiers in string format for all links that match\n",
    "        # our regex pattern from above\n",
    "        links    = {link['href'][30:] for link in content.find_all('a', href = True) if pattern.match(link['href'])} \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Provided a list of unique identifiers in string fromat, scrapes details and saves each entry \n",
    "as an idividual dataframe that represents one person.\n",
    "'''\n",
    "\n",
    "def scrape_details(uid, tor, headers):\n",
    "    cols = []\n",
    "    vals = []\n",
    "\n",
    "    url  = 'http://www.vdc-sy.info/index.php/en/details/martyrs/' + uid\n",
    "    \n",
    "    # Headers will provide the UserAgent to use when getting response\n",
    "    # Makes the request using a TorRequest object passed in\n",
    "    page = tor.get(url, headers = headers).text\n",
    "    page = bs(page, 'html.parser')\n",
    "    \n",
    "    # Grabs the relevant table info and all rows in it\n",
    "    table = page.find('table', attrs = {'class':'peopleListing'})\n",
    "    rows  = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "\n",
    "        # All data without only 2 data values\n",
    "        # are not data we are looking for\n",
    "        if len(data) != 2:\n",
    "            continue\n",
    "\n",
    "        # data[0] corresponds to the row label/column\n",
    "        cols.append(data[0].text)\n",
    "        \n",
    "        # Values need to appended differently for image rows \n",
    "        if data[1].find('img') is not None:\n",
    "            vals.append(data[1].find('img')['src'])\n",
    "        else:\n",
    "            vals.append(data[1].text)\n",
    "\n",
    "    # Adds the uid to the dataframe\n",
    "    cols.append('uid')\n",
    "    vals.append(uid)\n",
    "\n",
    "    # Creates and saves dataframe\n",
    "    person = pd.DataFrame([vals], columns = cols, dtype=str)\n",
    "\n",
    "    save(person, os.path.join('person_dfs', uid))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each detailed page has a different number of columns depending on the metadata associated with that entry, so we will now have to combine all the dataframes. Pandas requires that columns have unique names, so we have to rename all duplicate columns using this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dup_cols(dataframe):\n",
    "    cols = pd.Series(dataframe.columns)\n",
    "  \n",
    "    for dup in dataframe.columns.get_duplicates(): \n",
    "        cols[dataframe.columns.get_loc(dup)] = [dup + '_' + str(d_idx) if d_idx != 0 else dup for d_idx in range(dataframe.columns.get_loc(dup).sum())]\n",
    "   \n",
    "    dataframe.columns = cols\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given a list of dataframes we can return a combined dataframe that retains all column data and saves that file as vdc_df and saves any failed dataframes as failed_vdc_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes):\n",
    "    failed_dataframes = []\n",
    "    combined          = pd.DataFrame()\n",
    "\n",
    "    current = 0\n",
    "    num     = len(dataframes)\n",
    "\n",
    "    for df in dataframes:\n",
    "        try:\n",
    "            combined = pd.concat([combined, df], axis = 0)\n",
    "            print(f'{counter} / {num} people processed in combine_dataframes().')\n",
    "            counter += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_dataframes.append(df)\n",
    "            print('Failed')\n",
    "            counter += 1\n",
    "\n",
    "    save(combined, 'vdc_df')\n",
    "    save(failed_dataframes, 'failed_vdc_df')\n",
    "\n",
    "    print('\\n\\nSuccess: ', len(dataframes) - len(failed_dataframes))\n",
    "    print('Failed: ', len(failed_dataframes))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding this all together. We will now:\n",
    "\n",
    "1. Build a list of unique identifiers by scraping the query page for the VDC database using scrape_recent()\n",
    "\n",
    "2. Scrape the detailed information provided the list of unique ids from scrape_recent() using scrape_details, which gives us dataframes for each person.\n",
    "\n",
    "3. Combine those dataframes into one large dataset using combine_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_to_scrape = scrape_recent()\n",
    "uids_scraped   = set()\n",
    "\n",
    "while len(uids_to_scrape) > 0:\n",
    "    uid = uids_to_scrape.pop()\n",
    "    \n",
    "    try:\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        scrape_details(uid, tor, headers)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        helen___uids_to_scrape.append(uid)\n",
    "\n",
    "        ua         = UserAgent()\n",
    "        headers    = {'User-Agent': ua.random}\n",
    "        tor        = TorRequest(password = 'cmps184')\n",
    "        tor.reset_identity()\n",
    "\n",
    "        continue\n",
    "        \n",
    "    uids_scraped.add(uid)\n",
    "\n",
    "    save(uids_to_scrape, 'uids_to_scrape')\n",
    "    save(uids_scraped  , 'uids_scraped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "\n",
    "for person_df in glob.glob(os.path.join('person_dfs', '*.pickle')):\n",
    "    list_of_dataframes.append(load(person_df))\n",
    "    \n",
    "combine_dataframes(list_of_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally load in the dataset and look at its contents. For now, we are finished with the scraping portion for this source and we will revisit it when we will wrangle the data into a more suitable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdc_df = load('vdc_df')\n",
    "vdc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to see what the dataset looks like without any modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vdc_df = load('vdc_df')\n",
    "vdc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the added details we got from scraping everythign from the website are valuable for more detailed analysis, these particular columns will be what we will be focusing on with this project:\n",
    "\n",
    "- `Name                  `\n",
    "- `Status                `\n",
    "- `Sex                   `\n",
    "- `Province              `\n",
    "- `Area \\ Place of Birth `\n",
    "- `Date of death         `\n",
    "- `Cause of death        `\n",
    "- `Actors                `\n",
    "\n",
    "And we can create a dataframe we will use to do all of their data frame so that we are not modifying the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = vdc_df[['Province',\n",
    "                  'Sex',\n",
    "                  'Status',\n",
    "                  'Date of death',\n",
    "                  'Cause of Death']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `Sex` column, we can see that there is actually data about the person's minority status and age range, so we will create new columns to capture that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first want to drop any rows that don't have this information\n",
    "scratch = scratch.dropna(subset=['Sex'])\n",
    "\n",
    "def check_age(row):\n",
    "    if 'Adult' in row['Sex']:\n",
    "        val = 'adult'\n",
    "    else:\n",
    "        val = 'minor'\n",
    "    return val\n",
    "\n",
    "scratch['age_cat'] = scratch.apply(check_age, axis=1)\n",
    "\n",
    "def check_sex(row):\n",
    "    if 'Male' in row['Sex']:\n",
    "        val = 'male'\n",
    "    else:\n",
    "        val = 'female'\n",
    "    return val\n",
    "\n",
    "scratch['sex'] = scratch.apply(check_sex, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `Cause of Death` coolumn, we'll see that there is some reduntant categories, so we'll simplify these categories by remapping those values based on a dictionary mapping we show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_of_death_map = {'Chemical and toxic gases'         : 'Chemical Weapon',\n",
    "                      'Detention - Execution'            : 'Detention',\n",
    "                      'Detention - Torture'              : 'Detention',\n",
    "                      'Detention - Torture - Execution'  : 'Detention',\n",
    "                      'Explosion'                        : 'Explosion',\n",
    "                      'Field Execution'                  : 'Execution',\n",
    "                      'Kidnapping - Execution'           : 'Execution',\n",
    "                      'Kidnapping - Torture'             : 'Execution',\n",
    "                      'Kidnapping - Torture - Execution' : 'Execution',\n",
    "                      'Other'                            : 'Unknown'  ,\n",
    "                      'Shelling'                         : 'Shelling' ,\n",
    "                      'Shooting'                         : 'Shooting' ,\n",
    "                      'Siege'                            : 'Siege'    ,\n",
    "                      'Un-allowed to seek Medical help'  : 'Lack of Medical Access',\n",
    "                      'Unknown'                          : 'Unknown'  ,\n",
    "                      'Warplane shelling'                : 'Shelling' \n",
    "}\n",
    "\n",
    "def check_cause_of_death(row, mapping):\n",
    "    return mapping[row['Cause of Death']]\n",
    "\n",
    "scratch['cause_of_death'] = scratch.apply(check_cause_of_death,\n",
    "                                        args = (cause_of_death_map, ),\n",
    "                                        axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we can change the status column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(row):\n",
    "    if row['Status'] == 'Non-Civilian':\n",
    "        val = 'non_civilian'\n",
    "    elif row['Status'] == 'Civilian':\n",
    "        val = 'civilian'\n",
    "    else:\n",
    "        val = 'regime'\n",
    "    return val\n",
    "\n",
    "scratch['status'] = scratch.apply(check_status, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is cleaner, we can drop columns that irrelevant to us, and rename the columns for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = scratch[['Province',\n",
    "                  'sex',\n",
    "                  'status',\n",
    "                  'age_cat',\n",
    "                  'Date of death',\n",
    "                  'cause_of_death']].copy()\n",
    "\n",
    "scratch.columns = ['province', 'sex', 'status', 'age_cat','date_of_death', 'cause_of_death']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now drop any entries with unrecroded or icorrect dates of death and convert the time strings to python datetime objects.\n",
    "\n",
    "With all of those modifcations we can finally save this dataset as complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = scratch[scratch['province'].isin(picked)]\n",
    "scratch = scratch[scratch['date_of_death'] != '0000-00-00']\n",
    "scratch = scratch[scratch['date_of_death'] != '1970-01-01']\n",
    "scratch['date_of_death'] = pd.to_datetime(scratch['date_of_death'])\n",
    "\n",
    "save(scratch, 'clean_vdc_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not completely done, since we'd like to create and save some datasets that will be easy to use for visualization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vdc_df = load('clean_vdc_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casualty Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make sure that are working with the clean dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vdc_df = load('clean_vdc_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make visualualizations easy we'll write a function that let's us make bokeh-ready dataframes with a variety of options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Allows us to easily create Bokeh plots based oncategorical data in multiple time formats\n",
    "\n",
    "'''\n",
    "\n",
    "def make_bokeh_df(df, focus = 'province', tframe = 'year'):\n",
    "    \n",
    "    if tframe   == 'day'  :\n",
    "        bokeh_df         = df.groupby([focus , 'date_of_death']).agg({'date_of_death': 'count'})\n",
    "        bokeh_df.columns = ['count']\n",
    "        bokeh_df         = bokeh_df.reset_index()\n",
    "        bokeh_df.columns = [focus, 'day', 'count']\n",
    "        bokeh_df         = bokeh_df.pivot_table(index = ['day'], columns = focus, values = 'count').fillna(0)\n",
    "    \n",
    "    elif tframe == 'month':\n",
    "        bokeh_df = df.groupby([focus, (df.date_of_death.dt.year), (df.date_of_death.dt.month)]).agg({'date_of_death': 'count'})\n",
    "        bokeh_df.columns             = ['count']\n",
    "        bokeh_df.columns             = bokeh_df.columns.get_level_values(0)\n",
    "        bokeh_df.index.names         = [focus, 'year', 'month']\n",
    "        bokeh_df.reset_index(inplace = True)\n",
    "        bokeh_df                     = bokeh_df.pivot_table(index = ['year', 'month'], columns = focus, values = 'count').fillna(0)\n",
    "    \n",
    "    elif tframe == 'year' :\n",
    "        bokeh_df         = df.groupby([focus, (df.date_of_death.dt.year)]).agg({'date_of_death': 'count'})\n",
    "        bokeh_df.columns = ['count']\n",
    "        bokeh_df         = bokeh_df.reset_index()\n",
    "        bokeh_df.columns = [focus, 'year', 'count']\n",
    "        bokeh_df         = bokeh_df.pivot_table(index = ['year'], columns = focus, values = 'count').fillna(0)\n",
    "        \n",
    "    elif tframe == 'total' :\n",
    "        bokeh_df         = df.groupby([focus]).agg({'date_of_death': 'count'})\n",
    "        bokeh_df.columns = ['count']\n",
    "        bokeh_df         = bokeh_df.reset_index()\n",
    "        bokeh_df.columns = [focus, 'count']\n",
    "#         bokeh_df         = bokeh_df.pivot_table(index = [focus], columns = focus, values = 'count').fillna(0)\n",
    "        \n",
    "\n",
    "    return bokeh_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['province', 'sex', 'status', 'age_cat','date_of_death', 'cause_of_death']\n",
    "graph_type = ['line', 'point', 'bar', 'area', 'pie']\n",
    "time_frame = ['day', 'month', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refugee Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly Inflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly Refugee Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
